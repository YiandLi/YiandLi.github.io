<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":20,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1st Place Solution 0.933 Source: https:&#x2F;&#x2F;www.kaggle.com&#x2F;competitions&#x2F;kaggle-llm-science-exam&#x2F;discussion&#x2F;446422   Retrieval Model : mteb&#x2F;leaderboard e5-base-v2, e5-large-v2, gte-base, gte-large and bg">
<meta property="og:type" content="article">
<meta property="og:title" content="Kaggle_LLM_competition2024">
<meta property="og:url" content="http://example.com/2023/08/18/NLP/tricks/Kaggle_LLM_competition2024/index.html">
<meta property="og:site_name" content="Yili">
<meta property="og:description" content="1st Place Solution 0.933 Source: https:&#x2F;&#x2F;www.kaggle.com&#x2F;competitions&#x2F;kaggle-llm-science-exam&#x2F;discussion&#x2F;446422   Retrieval Model : mteb&#x2F;leaderboard e5-base-v2, e5-large-v2, gte-base, gte-large and bg">
<meta property="og:locale">
<meta property="og:image" content="https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F2675447%2Fe304b1553bae71f790a313f92ea3bc1b%2Fimage.png?generation=1697040115030343&amp;alt=media">
<meta property="article:published_time" content="2023-08-18T20:53:50.000Z">
<meta property="article:modified_time" content="2024-03-11T02:27:00.868Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F2675447%2Fe304b1553bae71f790a313f92ea3bc1b%2Fimage.png?generation=1697040115030343&amp;alt=media">

<link rel="canonical" href="http://example.com/2023/08/18/NLP/tricks/Kaggle_LLM_competition2024/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Kaggle_LLM_competition2024 | Yili</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yili</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Suggest Google Chrome for better math Reading.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/18/NLP/tricks/Kaggle_LLM_competition2024/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kaggle_LLM_competition2024
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-18 13:53:50" itemprop="dateCreated datePublished" datetime="2023-08-18T13:53:50-07:00">2023-08-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>967</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="1st-place-solution-0933"><a class="markdownIt-Anchor" href="#1st-place-solution-0933"></a> 1st Place Solution 0.933</h1>
<p>Source: <a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446422">https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446422</a></p>
<ul>
<li>
<p>Retrieval Model : <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/mteb/leaderboard">mteb/leaderboard</a><br />
e5-base-v2, e5-large-v2, gte-base, gte-large and bge-large</p>
</li>
<li>
<p>Retrieval Content -&gt; utilizes 2.5TB of input data</p>
<ul>
<li>Query: “{prompt} {A}, {B}, {C}, {D}, {E}” ; “{prompt} {A}”, “{prompt} {B}”, etc.</li>
<li>Key: using public wiki
<ul>
<li>recreated the above wiki taking a snapshot from a newer date</li>
<li>post processing steps: joining very small paragraphs (usually subtitles) with the subsequent paragraph</li>
<li>merged the sentences together to different target lengths of either 256, 512 or 1024 characters while not breaking any sentences.</li>
</ul>
</li>
<li>Custom pytorch cosine similarity code, no need for FAISS etc. - can just do it without any memory issue in chunks on GPU</li>
</ul>
</li>
<li>
<p>Longer contexts : train models with 3-chunk contexts and run inference with 5-chunks ; train faster</p>
</li>
<li>
<p>Train Models</p>
<ul>
<li>fine-tuned 7B &amp; 13B LLMs : Llama-2-7b ; Mistral-7B-v0.1 ; xgen-7b-8k-base ; Llama-2-13b</li>
<li>train all models with LORA on all linear layers</li>
<li>use a binary classification head after <strong>the final next-token logits</strong> ； with lower learning rate</li>
<li>use a lower learning rate for the head vs. the LORA layers</li>
<li>All models are trained over a single epoch with typical cosine lr decay and BCE loss, picking the last checkpoint.</li>
</ul>
</li>
</ul>
<p><img src="https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F2675447%2Fe304b1553bae71f790a313f92ea3bc1b%2Fimage.png?generation=1697040115030343&amp;alt=media" alt="" /></p>
<ul>
<li>Model Arch.
<ul>
<li>Since All the question share the same prefix , and prefix cannot see the question :  so feed the context and the question through the backbone and save the <code>past_key_values</code> .</li>
<li>each of the five answers gets the <code>past_key_values</code></li>
<li>take the final next token prediction of each output which are <strong>the logits over the full vocabulary</strong> and feed this through a final classification head trained from scratch.<br />
Map the output of the LLM across the full vocabulary to a single binary prediction.</li>
<li>Based on the score, get the final predicted option.</li>
<li>About the context :
<blockquote>
<p>One interesting test that we’ve done was to inverse the order of contexts during the inference which dropped the score quite a bit. So, it might mean that models learnt the position of the “best” context.<br />
Training on the true context the question was generated on, was a bit worse than training on the generated context.<br />
It helps the regularization and to learn that context may not always include the correct answers.<br />
“Unleashing” the model’s “knowledge” of Wikipedia</p>
</blockquote>
</li>
<li>Ensemble: Each model uses a different context approach using different wikis, embeddings and topk.</li>
</ul>
</li>
<li>Model Downside
<ul>
<li>each sample lacks the cross-information ； 即每一轮模型只是在考虑一个 question ， 无法同时考虑多个 question 。</li>
<li>作者也尝试了组合 question 进行拼接(<code>[CLS] &lt;question&gt; [SEP] &lt;retrieved context&gt; [SEP] &lt;option A&gt; [SEP]&lt;option B&gt; [SEP]&lt;option C&gt; [SEP]&lt;option D&gt; [SEP] &lt;option E&gt; [SEP]</code>)，但是发现效果都不好，因为存在 positional bias 。</li>
<li>solution:【late interaction】 最终的二分类模型的输入为，「logits over vocabulary of this option」 + 「the average next token logits over vocabulary of all other options」</li>
</ul>
</li>
</ul>
<h1 id="3rd-solution-0928"><a class="markdownIt-Anchor" href="#3rd-solution-0928"></a> 3rd Solution 0.928</h1>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446358">https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446358</a></p>
<h1 id="4th-place-solution-单模-0927"><a class="markdownIt-Anchor" href="#4th-place-solution-单模-0927"></a> 4th Place Solution 单模 0.927</h1>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446307">4th Place Solution</a><br />
<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/linshokaku/4th-elasticsearch-retrieval-example">4th elasticsearch retrieval example</a></p>
<p>Multiple Retrieval method:</p>
<ol>
<li>the score at the time of Elasticsearch search.</li>
<li>the edit distance with the question sentence.</li>
<li>semantic search, implemented using sentence-transformers.</li>
</ol>
<p>Validation:</p>
<ol>
<li>粗筛 + training set：At first stage, randomly extracted 2,000 instances as validation data from the open dataset without limiting to a specific domain.<br />
Implemented multiple retrieval rules and evaluated scores in a zero-shot manner to filter the retrieval solution. Using llama2-7b on the train set.</li>
<li>如果方法比较好，则使用 2000 instances out of 60K data 进行后续训练的 evaluation<br />
The raw <code>train.csv</code> was too easy for validation and was <strong>hardly</strong> useful; therefore, we did not use it much in the later stages.</li>
</ol>
<p>training：<br />
Model: Deberta v3 Large<br />
Datasets: Our main training datasets were 60k and 40k.<br />
Improvement in accuracy was noted by extending the number of tokens from 512 to 768 and 1280.</p>
<h1 id="5th-place-solution-0926"><a class="markdownIt-Anchor" href="#5th-place-solution-0926"></a> 5th Place Solution 0.926</h1>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446293">https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446293</a><br />
pyserini + SciBERT：<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/strifonov/pyserini-scibert">https://www.kaggle.com/code/strifonov/pyserini-scibert</a></p>
<ul>
<li>Sparse retrieval：BM25</li>
<li>Dense retrieval：
<ul>
<li>FAISS+instructor-xl：quantized the index</li>
<li>bge-large-en：STEM 270k</li>
</ul>
</li>
</ul>
<p><strong>Model :</strong> Llama-2-70b-hf and Mistral-7B-v0.1 , models without Instruction Tuning</p>
<p><strong>Inference :</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;context_0&#125;</span><br><span class="line"></span><br><span class="line">Question: &#123;prompt&#125;</span><br><span class="line">A. &#123;A&#125;</span><br><span class="line">B. &#123;B&#125;</span><br><span class="line">C. &#123;C&#125;</span><br><span class="line">D. &#123;D&#125;</span><br><span class="line">E. &#123;E&#125;</span><br><span class="line">Answer:</span><br></pre></td></tr></table></figure>
<p>Had the model output the probability that the token corresponding to “▁A”, “▁B”, “▁C”, “▁D”, “▁E” comes immediately after this sentence.</p>
<p>A test time augmentation strategy:<br />
LM can considerably change inference results depending on the order of choices, so rotate the order of choices 5 times to average this effect.</p>
<p><strong>Training :</strong></p>
<ul>
<li>QLoRA + 4bit quantization + PEFT’s LoRAModel</li>
<li>Registered quantized weights (less than 40GB) for each layer as a dataset, and performed inference layer-by-layer</li>
<li>Applying xformers’ <code>memory_efficient_attention</code> to the attention layers, we were able to keep memory consumption linear even with long contexts, leaving plenty of room in the GPU memory. We used only approximately 6GB of GPU memory for inference.</li>
</ul>
<p><strong>Pipeline :</strong> easy problems are answered by small models (mistral-7B) and difficult problems are answered by large models (llama2-70B).</p>
<ul>
<li>
<p>1st stage: ensemble of mistral-7B models<br />
mistral-7B with BM25 top2 contexts + mistral-7B with instructor and bge contexts<br />
Inference on all data</p>
</li>
<li>
<p>2nd stage: llama2-70B<br />
llama2-70B with BM25 top2, instructor and bge contexts<br />
Inference on bottom 40% data with low confidence in the 1st stage</p>
</li>
<li>
<p>3rd stage llama2-70B<br />
llama2-70B with bge contexts (2~3 times longer than bge contexts used in the 1st and 2nd stage)<br />
Inference on bottom 5% data with low confidence in the 1st and 2nd stage</p>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/08/13/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/Faiss/" rel="prev" title="Faiss">
      <i class="fa fa-chevron-left"></i> Faiss
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/08/26/%E5%A4%9A%E6%A8%A1%E6%80%81/multi-moddal%20backbones/Flamingo/" rel="next" title="Flamingo">
      Flamingo <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1st-place-solution-0933"><span class="nav-number">1.</span> <span class="nav-text"> 1st Place Solution 0.933</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3rd-solution-0928"><span class="nav-number">2.</span> <span class="nav-text"> 3rd Solution 0.928</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4th-place-solution-%E5%8D%95%E6%A8%A1-0927"><span class="nav-number">3.</span> <span class="nav-text"> 4th Place Solution 单模 0.927</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5th-place-solution-0926"><span class="nav-number">4.</span> <span class="nav-text"> 5th Place Solution 0.926</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">A foolish, slow learner.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">340</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YiandLi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YiandLi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yiliiiii" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yiliiiii" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">583k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">48:36</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
