<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":20,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="MQA，GQA StreamingLLM，H2O，SparQ lmdeploy">
<meta property="og:type" content="article">
<meta property="og:title" content="KV cache 显存优化">
<meta property="og:url" content="http://example.com/2023/10/01/NLP/LLM/KV_cache%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="Yili">
<meta property="og:description" content="MQA，GQA StreamingLLM，H2O，SparQ lmdeploy">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/Untitled.png">
<meta property="og:image" content="http://example.com/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/Untitled1.png">
<meta property="og:image" content="http://example.com/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/StreamUntitled.png">
<meta property="og:image" content="http://example.com/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/StreamUntitled1.png">
<meta property="og:image" content="http://example.com/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/h2oUntitled.png">
<meta property="article:published_time" content="2023-10-01T15:12:07.000Z">
<meta property="article:modified_time" content="2024-07-21T11:00:38.485Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/Untitled.png">

<link rel="canonical" href="http://example.com/2023/10/01/NLP/LLM/KV_cache%E4%BC%98%E5%8C%96/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>KV cache 显存优化 | Yili</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yili</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Suggest Google Chrome for better math Reading.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/01/NLP/LLM/KV_cache%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          KV cache 显存优化
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-10-01 23:12:07" itemprop="dateCreated datePublished" datetime="2023-10-01T23:12:07+08:00">2023-10-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/LLM-Inference/" itemprop="url" rel="index"><span itemprop="name">LLM Inference</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>14 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>MQA，GQA<br />
StreamingLLM，H2O，SparQ<br />
lmdeploy</p>
<span id="more"></span>
<h2 id="mqagqa"><a class="markdownIt-Anchor" href="#mqagqa"></a> MQA，GQA</h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/647130255">https://zhuanlan.zhihu.com/p/647130255</a></p>
<p>MHA（Multi-head Attention ： 标准的多头注意力机制，h个Query、Key 和 Value 矩阵。</p>
<p>MQA（<strong>Multi-Query Attention</strong>，Fast Transformer Decoding: One Write-Head is All You Need）</p>
<p>GQA（<strong>Grouped-Query Attention</strong>，GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints）</p>
<p>MQA（multi-query attention）让所有的 head 共享 1 个 KV projection 矩阵；</p>
<p>GQA（grouped-query attention ）使用 <strong>8 个 KV projections</strong>（选择8是因为A100 8GPUs） 来减少内存占用。</p>
<hr />
<p>首先是缓存机制，一旦模型规模很大长度很长时，<strong>KV 根本就存不进缓存</strong>。</p>
<p>比如 Llama 7B 模型，hidden size 是 4096，那么每个 timestep 需缓存参数量为 4096<em>2</em>32=262144，假设半精度保存就是 512KB，1024 长度那就要 512MB 。</p>
<p>于是退一步，放不进缓存可以放 DRAM 上去，而 DRAM 内存也就是我们常说的 GPU 显存。<br />
但 DRAM 读取到计算芯片和 SRAM 到计算芯片的速度，差了一个量级的，这会让计算芯片一直在等待。</p>
<p>同样，MQA 是一个软件层面上翻墙的一个方法。</p>
<p>MQA  让 <strong>Q 仍然保持原来的头数</strong>，但 <strong>K 和 V 只有一个头(mean)</strong>，不同head 之间共享同一个key矩阵和value矩阵。</p>
<p>即：预测一个 token，原本需要  8 头 query token 对 8 头 key / value token ；现在是 8 头 query token 对 1 头 key /  value token 。</p>
<p>这样 <strong>只需要传输 一个 head  的 token 就好，相当于减到 1/8 的传输量。实验发现，吞吐量提升 30 - 40  % ，即 通过减少内存访问压力来加速推理速度。</strong></p>
<p>实际上 <strong>MQA 运算量和 MHA 是差不多的</strong> ，可理解为 <strong>读取一组 KV 头</strong> 之后，<strong>给所有 Q 头用；但是传输量减少很多，同时也可以增加 batch - size 。</strong></p>
<p><img src="/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/Untitled.png" alt="alt text" /></p>
<p>而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。</p>
<p>具体思想是，不是所有 Q 头共享一组 KV，而是<strong>分组一定头数 Q 共享一组 KV</strong>，比如上面图片就是两组 Q 共享一组 KV。</p>
<p>如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA 论文里面一样，用已有的开源模型，挑一些头取<strong>个 mean 用来初始化</strong> MQA 或 GQA 继续训练一段时间。</p>
<ul>
<li>
<p>代码：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建原始的 multi head attention</span></span><br><span class="line">self.Wqkv = nn.Linear(    <span class="comment"># 【关键】Multi-Head Attention 的创建方法</span></span><br><span class="line">            self.d_model,</span><br><span class="line">            <span class="number">3</span> * self.d_model,  <span class="comment"># 有 query, key, value 3 个矩阵, 所以是 3 * d_model</span></span><br><span class="line">            device=device</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 【关键】每个 tensor 都是 (b_s, seq_len, hdden_size)</span></span><br><span class="line">query, key, value = qkv.chunk( <span class="number">3</span>,  dim=<span class="number">2</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==================================================================================================================</span></span><br><span class="line"><span class="comment"># 现在创建新的 Multi Query Attention</span></span><br><span class="line">self.Wqkv = nn.Linear(                  <span class="comment"># 【关键】Multi-Query Attention 的创建方法</span></span><br><span class="line">    self.d_model,</span><br><span class="line">    self.d_model + <span class="number">2</span> * self.head_dim,   <span class="comment"># 只创建 query 的 head 向量，所以只有 1 个 d_model</span></span><br><span class="line">    device=device,                      <span class="comment"># 而 key 和 value 不再具备单独的头向量</span></span><br><span class="line">)</span><br><span class="line">query, key, value = qkv.split(                        <span class="comment"># query -&gt; (b_s, seq_len, hdden_size)</span></span><br><span class="line">    [self.d_model, self.head_dim, self.head_dim],     <span class="comment"># key   -&gt; (b_s, seq_len, head_size)</span></span><br><span class="line">    dim=<span class="number">2</span>                                             <span class="comment"># value -&gt; (b_s, seq_len, head_size)</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后是广播进行 attention</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_multihead_dot_product_attention</span>(<span class="params"></span></span><br><span class="line"><span class="params">        query, <span class="comment"># (b_s, seq_len, hidden_size)</span></span></span><br><span class="line"><span class="params">        key,   <span class="comment"># (b_s, seq_len, head_size)</span></span></span><br><span class="line"><span class="params">        value, <span class="comment"># (b_s, seq_len, head_size)</span></span></span><br><span class="line"><span class="params">        n_heads,</span></span><br><span class="line"><span class="params">        multiquery=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">    q = rearrange(query, <span class="string">&#x27;b s (h d) -&gt; b h s d&#x27;</span>, h=n_heads)         <span class="comment"># (b_s, seq_len, hidden_size) -&gt; (b_s, head_num, seq_len, head_size)</span></span><br><span class="line">    kv_n_heads = <span class="number">1</span> <span class="keyword">if</span> multiquery <span class="keyword">else</span> n_heads</span><br><span class="line">    k = rearrange(key, <span class="string">&#x27;b s (h d) -&gt; b h d s&#x27;</span>, h=kv_n_heads)        <span class="comment"># (b_s, seq_len, hidden_size) -&gt; (b_s, head_num, head_size, seq_len) if not multiquery</span></span><br><span class="line">                                                                    <span class="comment"># (b_s, seq_len, head_size) -&gt; (b_s, 1, head_size, seq_len)  if multiquery</span></span><br><span class="line"></span><br><span class="line">    v = rearrange(value, <span class="string">&#x27;b s (h d) -&gt; b h s d&#x27;</span>, h=kv_n_heads)      <span class="comment"># (b_s, seq_len, hidden_size) -&gt; (b_s, head_num, seq_len, head_size) if not multiquery</span></span><br><span class="line">                                                                    <span class="comment"># (b_s, seq_len, head_size) -&gt; (b_s, 1, seq_len, head_size)  if multiquery</span></span><br><span class="line"></span><br><span class="line">    attn_weight = q.matmul(k) * softmax_scale                       <span class="comment"># (b_s, head_num, seq_len, seq_len)</span></span><br><span class="line">    attn_weight = torch.softmax(attn_weight, dim=-<span class="number">1</span>)                <span class="comment"># (b_s, head_num, seq_len, seq_len)</span></span><br><span class="line"></span><br><span class="line">    out = attn_weight.matmul(v)                                     <span class="comment"># (b_s, head_num, seq_len, seq_len) * (b_s, 1, seq_len, head_size) = (b_s, head_num, seq_len, head_size)</span></span><br><span class="line">    out = rearrange(out, <span class="string">&#x27;b h s d -&gt; b s (h d)&#x27;</span>)                    <span class="comment"># (b_s, seq_len, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, attn_weight, past_key_value</span><br></pre></td></tr></table></figure>
<p>每一个 token 变成：</p>
<p>原始维度的 query（768 = 8 * 96），保留头数</p>
<p>单一头对应的 key 和 value  ： 96</p>
<p>所以总共为</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q">为什么现在大家都在用 MQA 和 GQA？</a></p>
<h2 id="kv-窗口优化"><a class="markdownIt-Anchor" href="#kv-窗口优化"></a> <strong>KV 窗口优化</strong></h2>
<p>KV cache 一般需要缓存前面的所有 token ，当长度过长时，缓存量会变得很大。</p>
<p><img src="/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/Untitled1.png" alt="alt text" /></p>
<p>如果 prompt 长度为 L ，现在需要预测 T 个 token（ T&gt;&gt;L ） 。</p>
<p>一个自然的想法就是滑动窗口。这里有：</p>
<ol>
<li>
<p>Window Attention (figure b) → LongFormer ：</p>
<p>优点是降低显存占用量（线性），提升推理速度</p>
<p>但是一旦序列长度超过了缓存大小，也就是开始删除KV，模型的效果就会急剧下降。</p>
</li>
<li>
<p>KV 重计算 ：每次计算都需要重新计算最近的长度为 L 的 KV cache 。由于重计算的存在，其精度可以保证，但是其速度明显较慢，在现实应用中并不实用。</p>
</li>
<li>
<p>箭型 attention 窗口 ：在 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2308.16137.pdf">LM-Infinit</a> 中就已经被提出了，其基本原理和 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2309.17453.pdf">StreamingLLM</a> 是一致的。</p>
<p><a href="KV%20cache%20%E4%BC%98%E5%8C%96%205b076c8d3b414859a5b6d9e400902e1b/StreamingLLM%20Efficient%20Streaming%20Language%20Models%20w%20a6cecec0eb0045098618d45b9ebec1ef.md"><strong>StreamingLLM : Efficient Streaming Language Models with Attention Sinks</strong></a></p>
</li>
</ol>
<p><strong>稀疏 KV</strong></p>
<ol>
<li>
<p>H2O: 通过动态的评价方式来判断需要保留和废弃的KV值</p>
<p><a href="KV%20cache%20%E4%BC%98%E5%8C%96%205b076c8d3b414859a5b6d9e400902e1b/H2O%20Heavy-Hitter%20Oracle%20for%20Efficient%20Generative%20I%20fd3f7ba5de8f4d4b88fbefb2ce4df2a5.md"><strong>H</strong>2<strong>O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</strong></a></p>
</li>
<li>
<p>SparQ Attention: 仅仅保留 top-scoring key and value pairs，减少缓存的 kv 数量</p>
<p><a href="KV%20cache%20%E4%BC%98%E5%8C%96%205b076c8d3b414859a5b6d9e400902e1b/SparQ%20Attention%20Bandwidth-Efficient%20LLM%20Inference%20c043ad91d0f04a9db8f5e25ee7d9a2c8.md"><strong>SparQ Attention: Bandwidth-Efficient LLM Inference.</strong></a></p>
</li>
<li>
<p>Dist Attention : <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.02669.pdf">https://arxiv.org/pdf/2401.02669.pdf</a></p>
</li>
<li>
<p><strong>Generating Long Sequences with Sparse Transformers ：</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.10509">https://arxiv.org/abs/1904.10509</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/679559231">https://zhuanlan.zhihu.com/p/679559231</a></p>
</li>
</ol>
<h3 id="streamingllm-efficient-streaming-language-models-with-attention-sinks"><a class="markdownIt-Anchor" href="#streamingllm-efficient-streaming-language-models-with-attention-sinks"></a> StreamingLLM : Efficient Streaming Language Models with Attention Sinks</h3>
<p><a target="_blank" rel="noopener" href="https://www.high-flyer.cn/en/blog/streamingllm/">https://www.high-flyer.cn/en/blog/streamingllm/</a></p>
<ol>
<li>长度外推：enables LLMs trained with a <em>finite length</em> attention window to generalize to <em>infinite sequence length</em> without any fine-tuning.</li>
<li>在预训练时，使用一个 placeholder token 作为 attention sink ，能够进一步提升 streaming deployment 。</li>
</ol>
<p>作者发现，在自回归生成中，大量 Attention 权重总会被分配给某些初始 token ，无论这些 token 与语言建模任务是否相关。</p>
<p>尽管这些初始 token 缺乏语义，但它们收集了显著的注意力分数。</p>
<p><img src="/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/StreamUntitled.png" alt="alt text" /></p>
<p>如上图，仅仅 layer 0 和 1 中，注意力分数集中在相近的前缀 token 上，其他层都明显表现出了 initial token 占据很大 attention 的现象。</p>
<p>作者将这些 initial tokens 称为 <strong>“attention sink”</strong> 。</p>
<p>研究者将这个现象归因于计算中的 Softmax 操作，它要求注意力分数总和为 1 :</p>
<p>即使当前的 input token 和先前的 tokens 没有强匹配，模型仍然需要分配这些不需要的 attention score，以使最终的注意力分数总和为 1 。</p>
<p>由于自回归语言建模的顺序性质，Initial tokens 可见于所有 tokens ，而 later tokens 只能被一组有限的 subsequent tokens 看到。</p>
<p>因此，初始标记更容易被训练成为注意力吸引点，捕获不必要的注意力。</p>
<h4 id="方法"><a class="markdownIt-Anchor" href="#方法"></a> 方法</h4>
<p>借助上面的观察，研究者们提出了 StreamingLLM，一种简单而高效的框架。如图1 d 所示，该模型就是把初始 token 的 KV 保留下来，然后再拼上滑动窗口的 KV，就可以保持稳定的模型性能，因为保留 attention sinks 可以让窗口内的注意力分数分布接近正常。</p>
<h5 id="窗口注意力机制失败的原因"><a class="markdownIt-Anchor" href="#窗口注意力机制失败的原因"></a> 窗口注意力机制失败的原因</h5>
<p>实验发现，当 KV cache 删除了 initial tokens ，困惑度会提升。这表明，无论初始词块与被预测词块的距离如何，初始词块对于保持 LLM 的稳定性都至关重要。</p>
<p>这是因为，初始 Token 占据很多 attention score，<strong>「删除这些初始 token 的 KV 」<strong>将去掉 SoftMax 函数中分母的很大一部分。这种改变会导致注意力分数的</strong>分布</strong>发生显著变化</p>
<h5 id="为什么-initial-tokens-重要"><a class="markdownIt-Anchor" href="#为什么-initial-tokens-重要"></a> 为什么 initial tokens 重要？</h5>
<p>研究者给出了如下两种可能：</p>
<ol>
<li>初始 token 的语义至关重要；</li>
<li>或者模型学习偏重于初始 token 的绝对位置</li>
</ol>
<p>研究者用“\n”替代前四个标记符，发现模型仍然会非常重视初始换行符。这表明，初始token的绝对位置比其语义价值更加重要。</p>
<p>同时，作者观察到，LLMs通常被训练为利用多个初始标记作为 attention sink，而不仅仅是一个。</p>
<p>作者认为这种模式的出现是因为这些模型在预训练期间没有在所有输入样本中包含 <strong>一致的起始标记</strong> 。</p>
<p>虽然Llama-2确实在每个段落前加上了一个起始token标记，但是它是在 text chunking 之前应用的，导致大部分 random token 占据了零位置。</p>
<p>这种 lack of a uniform starting token 导致模型使用了several initial tokens 作为 attention sinks 。</p>
<p>因此，作者希望通过在所有训练数据前增加一个 a stable learnable token  ，作为一个 attention sink 。</p>
<h4 id="具有-attention-sinks-的滑动-kv-缓存-in-already-trained-llms"><a class="markdownIt-Anchor" href="#具有-attention-sinks-的滑动-kv-缓存-in-already-trained-llms"></a> <strong>具有 attention sinks 的滑动 KV 缓存 （</strong> in already trained LLMs</h4>
<p><img src="/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/StreamUntitled1.png" alt="" /></p>
<ol>
<li>
<p>Attention sinks ( <strong>four</strong> initial tokens ) stabilize the attention computation</p>
</li>
<li>
<p>Rolling KV Cache retains the most recent tokens</p>
</li>
<li>
<p>focuses on positions <em>within the cache</em> rather than those <em>in the original text</em>.</p>
</li>
</ol>
<p>例如，如果当前缓存的 token 为 [0, 1, 2, 3, 6, 7, 8]，那么在解码第 9 个令牌的过程中，分配的位置为 [0, 1, 2, 3, 4, 5, 6, 7]，而不是原始文本中的位置 [0, 1, 2, 3, 6, 7, 8, 9]。</p>
<p>这也就是为什么可以无限长度外推，用了上述方法，GPU 的显存上限是确定的，即可保证长度的“无限”增加。</p>
<h4 id="带有-attention-sinks-的预训练-llms-pre-training"><a class="markdownIt-Anchor" href="#带有-attention-sinks-的预训练-llms-pre-training"></a> <strong>带有 attention sinks 的预训练 LLMs ( PRE-TRAINING</strong></h4>
<ul>
<li>
<p>方法 1 <strong>“Sink Token”</strong>：为所有训练样本加入 <strong>a global trainable attention sink token</strong> ，称为 <strong>“Sink Token”</strong>，作为不必要注意力分数的存放处。</p>
</li>
<li>
<p>方法 2 <strong>“Zero Sink”</strong>  ：也可以用类似于 SoftMax-off-by-One 的变体来取代传统的 SoftMax 函数：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo><mi mathvariant="normal">SoftMax</mi><mo>⁡</mo></mo><mn>1</mn></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><mn>1</mn><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\operatorname{SoftMax}_1(x)_i=\dfrac{e^{x_i}}{1+\sum_{j=1}^N e^{x_j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">S</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">M</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.648441em;vertical-align:-1.3070490000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.128769em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3070490000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>不要求所有上下文 token 的注意力分数总和为 1。</p>
<p>这种替代方法等同于在注意力计算中使用 KV 特征全为零的 token 。作者将这种方法命名为 <strong>“Zero Sink”</strong>，以便与其提出的框架保持一致。</p>
</li>
</ul>
<h3 id="h2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models"><a class="markdownIt-Anchor" href="#h2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models"></a> H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</h3>
<p>代码：<a target="_blank" rel="noopener" href="http://eadst.com/blog/222">http://eadst.com/blog/222</a></p>
<p>通过动态的评价方式来判断需要保留和废弃的KV值</p>
<p>作者发现：仅仅有少部分的 token 占据了 attention score，即起激活作用，实验发现 5% 的 histrial token 就足够预测出正确的答案了，这部分 token 被称为 <strong>Heavy Hitters</strong> 。</p>
<h4 id="greedy-algorithm-for-low-cost-policy"><a class="markdownIt-Anchor" href="#greedy-algorithm-for-low-cost-policy"></a> <strong>Greedy Algorithm for Low-Cost Policy</strong></h4>
<p><strong>方法：at every decoding step , using local statistics by summing up the attention scores of the previous tokens</strong></p>
<p>在第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathnormal">i</span></span></span></span>   轮时，kv cache 的 token id 集合为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">S_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>我们定义  H2O Eviction Policy ：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>→</mo><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">g : S_{i-1}\rightarrow S_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，即使用上一轮的 kv cache 更新得到这一轮的 kv cache 。</p>
<p>要求：</p>
<ul>
<li>kv cache 缓存的 token 数量为固定的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>  : <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>S</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">|Si| = k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>i</mi></msub><mo>−</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|S_i - S_{i-1} | \le 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> ，每轮最多删除一个 token 的 kv cache</li>
</ul>
<p>更新策略为，将输入 token i 先拼接入 kv cache 中，得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">S_{i-1} \cup\{i\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal">i</span><span class="mclose">}</span></span></span></span> ，然后在这个集合中选择 attention 分数最小的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span>  ，删除 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span> ：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>←</mo><mrow><mo fence="true">(</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo><mo fence="true">)</mo></mrow><mi mathvariant="normal">\</mi><mo stretchy="false">{</mo><mi>u</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">S_i \leftarrow\left(S_{i-1} \cup\{i\}\right) \backslash\{u\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">{</span><span class="mord mathnormal">i</span><span class="mclose">}</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">\</span><span class="mopen">{</span><span class="mord mathnormal">u</span><span class="mclose">}</span></span></span></span>   as  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>←</mo><mi>arg</mi><mo>⁡</mo><msub><mo><mi>max</mi><mo>⁡</mo></mo><mrow><mi>v</mi><mo>∈</mo><mrow><mo fence="true">(</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo><mo fence="true">)</mo></mrow></mrow></msub><msub><mi>F</mi><mtext>score </mtext></msub><mrow><mo fence="true">(</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo><mi mathvariant="normal">\</mi><mo stretchy="false">{</mo><mi>v</mi><mo stretchy="false">}</mo><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">u \leftarrow \arg \max _{v \in\left(S_{i-1} \cup\{i\}\right)} F_{\text {score }}\left(S_{i-1} \cup\{i\} \backslash\{v\}\right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mrel mtight">∈</span><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32808571428571426em;"><span style="top:-2.357em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20252142857142857em;"><span></span></span></span></span></span></span><span class="mbin mtight">∪</span><span class="mopen mtight">{</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">}</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">)</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">score </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">{</span><span class="mord mathnormal">i</span><span class="mclose">}</span><span class="mord">\</span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">}</span><span class="mclose delimcenter" style="top:0em;">}</span></span></span></span></span></p>
<p>总之就是每次减去一个相似度最小的 token</p>
<p><img src="/images/NLP/LLM/KVcache%E4%BC%98%E5%8C%96/h2oUntitled.png" alt="" /></p>
<h3 id="sparq-attention-bandwidth-efficient-llm-inference"><a class="markdownIt-Anchor" href="#sparq-attention-bandwidth-efficient-llm-inference"></a> SparQ Attention: Bandwidth-Efficient LLM Inference.</h3>
<p>缺点：</p>
<p>&quot;SparQ Attention has some limitations: while maintaining very strong performance at high bandwidth compression ratios, this is achieved by keeping all cached values in memory which is sparsely accessed during the generation steps.</p>
<p>It therefore does not save any memory capacity, only bandwidth. Another possible limitation is the unclear efficiency saving of SparQ Attention when used with transformer models using MQA and GQA, which were not evaluated in this work. We leave it as future work to extend SparQ Attention to cover more attention mechanisms&quot;</p>
<h2 id="量化"><a class="markdownIt-Anchor" href="#量化"></a> <strong>量化</strong></h2>
<p>Quantized KV Cache ：</p>
<p>比如框架 <a href="https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy">lmdeploy</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/09/11/machine_learninng/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B7%9D%E7%A6%BB%E5%85%AC%E5%BC%8F/" rel="prev" title="逻辑回归和线性回归">
      <i class="fa fa-chevron-left"></i> 逻辑回归和线性回归
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/02/Coding/Polars/ploars-01-intro/" rel="next" title="ploars_01_intro">
      ploars_01_intro <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#mqagqa"><span class="nav-number">1.</span> <span class="nav-text"> MQA，GQA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kv-%E7%AA%97%E5%8F%A3%E4%BC%98%E5%8C%96"><span class="nav-number">2.</span> <span class="nav-text"> KV 窗口优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#streamingllm-efficient-streaming-language-models-with-attention-sinks"><span class="nav-number">2.1.</span> <span class="nav-text"> StreamingLLM : Efficient Streaming Language Models with Attention Sinks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.1.1.</span> <span class="nav-text"> 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AA%97%E5%8F%A3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="nav-number">2.1.1.1.</span> <span class="nav-text"> 窗口注意力机制失败的原因</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-initial-tokens-%E9%87%8D%E8%A6%81"><span class="nav-number">2.1.1.2.</span> <span class="nav-text"> 为什么 initial tokens 重要？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E6%9C%89-attention-sinks-%E7%9A%84%E6%BB%91%E5%8A%A8-kv-%E7%BC%93%E5%AD%98-in-already-trained-llms"><span class="nav-number">2.1.2.</span> <span class="nav-text"> 具有 attention sinks 的滑动 KV 缓存 （ in already trained LLMs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%A6%E6%9C%89-attention-sinks-%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83-llms-pre-training"><span class="nav-number">2.1.3.</span> <span class="nav-text"> 带有 attention sinks 的预训练 LLMs ( PRE-TRAINING</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#h2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models"><span class="nav-number">2.2.</span> <span class="nav-text"> H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#greedy-algorithm-for-low-cost-policy"><span class="nav-number">2.2.1.</span> <span class="nav-text"> Greedy Algorithm for Low-Cost Policy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparq-attention-bandwidth-efficient-llm-inference"><span class="nav-number">2.3.</span> <span class="nav-text"> SparQ Attention: Bandwidth-Efficient LLM Inference.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8F%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text"> 量化</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">A foolish, slow learner.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">352</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YiandLi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YiandLi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yiliiiii" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yiliiiii" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">602k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">50:12</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
