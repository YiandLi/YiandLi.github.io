<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":20,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A foolish, slow learner.">
<meta property="og:type" content="website">
<meta property="og:title" content="Yili">
<meta property="og:url" content="http://example.com/page/25/index.html">
<meta property="og:site_name" content="Yili">
<meta property="og:description" content="A foolish, slow learner.">
<meta property="og:locale">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/25/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Yili</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yili</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Suggest Google Chrome for better math Reading.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/2.%20Datasets%E5%92%8CDataloaders/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/2.%20Datasets%E5%92%8CDataloaders/" class="post-title-link" itemprop="url">2. Dataset, Sampler, Dataloader</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:46:58" itemprop="dateCreated datePublished" datetime="2021-09-22T23:46:58+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>913</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>torch.utils.data.Dataset</code>对象：存储样本信息。<br />
<code>torch.utils.data.Sampler</code>对象：提供一种遍历数据集所有元素索引的方式。<br />
<code>torch.utils.data.DataLoader</code>对象：包装一个Dataset的迭代器，用于更加方便地得到样本。</p>
<ul>
<li>总地来说：<br />
Dataloader 负责总的调度，<br />
命令 Sampler 定义遍历索引的方式，<br />
然后用索引去 Dataset 中提取元素。<br />
于是就实现了对给定数据集的遍历。</li>
</ul>
<h2 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h2>
<p><code>torch.utils.data.Dataset</code>对象有两种：「Map-style dataset」和 「Iterable-style dataset」。<br />
每个样本有对应的索引，通过输入具体的索引，就能够得到自定义的样本信息。</p>
<h3 id="map-style-dataset"><a class="markdownIt-Anchor" href="#map-style-dataset"></a> Map-style dataset</h3>
<p>自定义的Dataset对象必须实现<code>__init__()</code>, <code>__len__()</code>和<code>__getitem__(self, idx)</code>方法，作为一种存储和读取数据的协议。</p>
<p>「Map-style dataset」并不是直接读取的数据集，而是读取每个样本的索引。<br />
即后续取样本时，通过借助索引，即<code>__getitem__()</code>协议的方式读取样本。大致流程为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for indices in batch_sampler:</span><br><span class="line">    yield collate_fn([dataset[i] for i in indices])</span><br></pre></td></tr></table></figure>
<h4 id="参数"><a class="markdownIt-Anchor" href="#参数"></a> 参数：</h4>
<ul>
<li><code>__init__()</code>：在创建dataset时被调用一次</li>
<li><code>__len__()</code>：返回样本数量</li>
<li><code>__getitem__(self, idx)</code>：加载并且返回一个下标为<code>idx</code>的样本。<br />
对应的操作符就是索引符号<code>[]</code>，即<code>dataset.__getitem__(idx)</code>和<code>dataset[idx]</code>操作一致。</li>
</ul>
<h3 id="iterable-style-dataset"><a class="markdownIt-Anchor" href="#iterable-style-dataset"></a> Iterable-style dataset</h3>
<p>并不需要实现<code>__getitem__()</code>方法和<code>__len__()</code>方法。Dataset本身更像是一个迭代器，使用<code>__iter()__</code>协议。<br />
不同于「map-style dataset」，该类型数据集下样本的索引相互独立，注意使用 <code>DataLoader</code> 载入数据时，如果 <code>num_worker</code> 数量大于1，则需要独立分配索引。</p>
<h2 id="dataloader"><a class="markdownIt-Anchor" href="#dataloader"></a> DataLoader</h2>
<ul>
<li><code>Dataset</code>一次只能检索到一个样本，但是我们普遍以batch为单位进行训练。所以我们使用<code>DataLoader</code>作为一个迭代器，返回batch单位的数据。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)</span><br></pre></td></tr></table></figure>
<ul>
<li>DataLoader的每一次迭代过程都会返回一个batch大小的样本特征数组和样本标签数组。<br />
<code>train_features, train_labels = next(iter(train_dataloader))</code></li>
</ul>
<p>定义后查看一个 batch内部数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># have a look at dataloader</span><br><span class="line"></span><br><span class="line">train_data_iter = iter(train_dataloader)</span><br><span class="line">batch_data = next(train_data_iter)</span><br><span class="line">text_id_list, text_list, batch_input_ids, \</span><br><span class="line">batch_attention_mask, batch_token_type_ids, \</span><br><span class="line">tok2char_span_list, batch_shaking_tag = batch_data</span><br></pre></td></tr></table></figure>
<h3 id="参数-2"><a class="markdownIt-Anchor" href="#参数-2"></a> 参数</h3>
<ul>
<li><code>shuffle=True</code>时，调用<code>RandomSampler</code>进行随机索引。</li>
<li><code>sampler</code>定义从数据集中提取样本的策略，应该传入一个<code>Sampler</code>对象。如果指定<code>sampler</code>对象，则<code>shuffle</code>参数必须为<code>False</code>。</li>
<li><code>batch_sampler</code>，和<code>sampler</code>类似，但是一般传入<code>BatchSampler</code>对象，每次返回一个<code>batch</code>大小的索引。和<code>batch_size</code>，<code>shuffle</code>参数互斥。</li>
<li><code>drop_last=True</code>时，会自动删除最后一个不完整的batch。</li>
<li><code>collate_fn=&lt;function default_collate&gt;</code>，定义如何取样本，我们可以定义自己的函数来准确地实现想要的功能。<br />
从「Map-style dataset」处取样本的伪代码可以看到，<code>collate_fn</code>这个函数有一个默认的输入<code>[dataset[i] for i in indices]</code>，list的长度是一个batch size，list中的每个元素都是<code>__getitem__()</code>得到的结果。所以在重写该方法时，注意有一个默认的参数<code>def collate(self, examples)</code>。</li>
<li>·<code>num_workers</code>：使用多少个进行处理数据，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/523239005">使用unix系统时请注意BUG</a>。</li>
</ul>
<h2 id="demo"><a class="markdownIt-Anchor" href="#demo"></a> Demo</h2>
<ol>
<li>使用一个自定义的<code>read_example_form_file(args, mode)</code> 方法，读取所有的样本，生成一个样本列表。<br />
样本列表的结构为：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">examples = [</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;sentence&quot;: sentence,</span><br><span class="line">        &quot;label&quot;: label,</span><br><span class="line">        &quot;feature&quot;: feature</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>
<p>自定义<code>my_Dataset</code>继承<code>torch.utils.data.Dataset</code>，</p>
<ol>
<li>实现<code>__init__()</code>，<code>__len_()</code>，<code>__getitem__()</code>方法。</li>
<li>自定义一个<code>collate(self, example_list)</code>方法，后续直接传入<code>DataLoader</code>的<code>collate_fn</code>参数。<br />
在<code>collate()</code>内部实现 tokenizer 和 padding 操作，返回<code>raw_text_list, batch_input_ids, batch_attention_mask, batch_segment_ids, batch_labels, batch_hand_features</code>，这样训练时，直接取出即可交给模型，计算损失，进行训练。</li>
</ol>
</li>
<li>
<p>直接使用<code>torch.utils.data.DataLoader</code>封装<code>my_Dataset</code>和<code>my_Dataset.collate</code>即可。<br />
注意传入的是<code>collate</code>(<code>&lt;function&gt;</code>)，不是<code>collate()</code>。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.4.%20Torch.tensor%E6%8B%BC%E6%8E%A5%E5%92%8C%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.4.%20Torch.tensor%E6%8B%BC%E6%8E%A5%E5%92%8C%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">1.4. Torch.tensor 拼接和分割</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:38" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:38+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>365</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="堆砌拼接"><a class="markdownIt-Anchor" href="#堆砌拼接"></a> 堆砌拼接</h2>
<h3 id="stack"><a class="markdownIt-Anchor" href="#stack"></a> stack()</h3>
<p>在一个新的维度上拼接，注意会<strong>生成新的维度</strong>。</p>
<p>参数</p>
<ul>
<li><code>tensors (sequence of Tensors)</code>：tensor 列表，所有 tensor 的维度大小必须一致。</li>
<li><code>dim=0 (int)</code>：拼接的维度</li>
</ul>
<h3 id="cat"><a class="markdownIt-Anchor" href="#cat"></a> cat()</h3>
<p>在指定维度上拼接，<strong>不会生成新的维度</strong>。</p>
<p>参数</p>
<ul>
<li><code>tensors (sequence of Tensors)</code>：tensor 列表，所有 tensor ，除了待拼接的维度外，其他维度的大小必须一致。</li>
<li><code>dim=0 (int, optional)</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.stack([a,b])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.stack([a,b])</span><br><span class="line">tensor([[[1., 1., 1.],</span><br><span class="line">         [1., 1., 1.]],</span><br><span class="line">        [[0., 0., 0.],</span><br><span class="line">         [0., 0., 0.]]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.cat([a,b])</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>
<h2 id="分割"><a class="markdownIt-Anchor" href="#分割"></a> 分割</h2>
<h3 id="split"><a class="markdownIt-Anchor" href="#split"></a> split()</h3>
<p>分割tensor，返回分割后的子tensor列表。</p>
<p>参数：</p>
<ul>
<li><code>tensor (Tensor)</code></li>
<li><code>split_size_or_sections (int) or (list(int))</code>
<ul>
<li>如果是整数<code>split_size</code>，则分成多个维度为<code>split_size</code>的tensor。（如果不能均分，则最后一个子tensor的维度会小于<code>split_size</code>）。</li>
<li>如果是列表<code>split(list(int))</code>，则按照列表的元素大小来分割。</li>
</ul>
</li>
<li><code>dim=0 (int)</code>：分割维度</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.arange(10).reshape(5,2)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [2, 3],</span><br><span class="line">        [4, 5],</span><br><span class="line">        [6, 7],</span><br><span class="line">        [8, 9]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.split(a, 2)</span><br><span class="line">(tensor([[0, 1],</span><br><span class="line">         [2, 3]]),</span><br><span class="line"> tensor([[4, 5],</span><br><span class="line">         [6, 7]]),</span><br><span class="line"> tensor([[8, 9]]))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.split(a, [1,4])</span><br><span class="line">(tensor([[0, 1]]),</span><br><span class="line"> tensor([[2, 3],</span><br><span class="line">         [4, 5],</span><br><span class="line">         [6, 7],</span><br><span class="line">         [8, 9]]))</span><br></pre></td></tr></table></figure>
<h3 id="chunk"><a class="markdownIt-Anchor" href="#chunk"></a> chunk()</h3>
<p>同样是分割函数，和<code>split()</code>方法类似，区别在于传入参数<code>chunks</code>，不同于参数<code>split_size_or_sections</code>。</p>
<p>参数</p>
<ul>
<li><code>tensor (Tensor)</code></li>
<li><code>chunks (int)</code>：返回的分割tensor的数量，如果无法整除，则最后一个子tensor的维度为余数。</li>
<li><code>dim=0 (int)</code>：分割维度</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.5.%20Torch.tensor%E6%89%A9%E5%A2%9E%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%B9%BF%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.5.%20Torch.tensor%E6%89%A9%E5%A2%9E%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%B9%BF%E6%92%AD/" class="post-title-link" itemprop="url">1.5. Torch.tensor扩增，压缩和广播</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:38" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:38+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>851</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>对任意一个tensor来说，其大小等于所有维度大小的乘积。<br />
所以任意增加或者减少一个大小为1的维度，不会改变原始大小。</p>
<h2 id="unsqueeze"><a class="markdownIt-Anchor" href="#unsqueeze"></a> unsqueeze()</h2>
<p>增加张量的维度<br />
不会新建tensor，只是返回一个新的视图。<br />
结果tensor和原tensor的内存是共享的，如果后续需要修改可以使用<code>contiguous()</code>方法。</p>
<p>参数</p>
<ul>
<li><code>input (Tensor)</code>：</li>
<li><code>dim (int)</code>：<code>[-input.dim()-1, input.dim()+1)</code>支持负数</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.unsqueeze(x, 0)</span><br><span class="line">tensor([[ 1,  2,  3,  4]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.unsqueeze(x, 1)</span><br><span class="line">tensor([[ 1],</span><br><span class="line">        [ 2],</span><br><span class="line">        [ 3],</span><br><span class="line">        [ 4]])</span><br></pre></td></tr></table></figure>
<h2 id="squeeze"><a class="markdownIt-Anchor" href="#squeeze"></a> squeeze()</h2>
<p>压缩：删除大小为1的维度<br />
不会新建tensor，只是返回一个新的视图。<br />
结果tensor和原tensor的内存是共享的，如果后续需要修改可以使用<code>contiguous()</code>方法。</p>
<p>参数</p>
<ul>
<li><code>input (Tensor)</code></li>
<li><code>dim=None (int, optional)</code>
<ul>
<li>如果为<code>None</code>，则删除所有大小为1的维度</li>
<li>如果不为<code>None</code>，则删除选定的维度。如果选定维度大小不为1，则unchanged。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([2, 1, 2, 1, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.unsqueeze(-1).data_ptr() == x.data_ptr() # 内存共享</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; y = torch.squeeze(x)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([2, 2, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; y = torch.squeeze(x, 0)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([2, 1, 2, 1, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; y = torch.squeeze(x, 1)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([2, 2, 1, 2])</span><br></pre></td></tr></table></figure>
<h2 id="tensorexpand"><a class="markdownIt-Anchor" href="#tensorexpand"></a> tensor.expand()</h2>
<p>不会新建tensor，只是返回一个新的视图。<br />
结果tensor和原tensor的内存是共享的，如果后续需要修改可以使用<code>contiguous()</code>方法。</p>
<p>参数</p>
<ul>
<li><code>*sizes (torch.Size or int...)</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([3, 1])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.expand(3, 4)</span><br><span class="line">tensor([[ 1,  1,  1,  1],</span><br><span class="line">        [ 2,  2,  2,  2],</span><br><span class="line">        [ 3,  3,  3,  3]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension</span><br><span class="line">tensor([[ 1,  1,  1,  1],</span><br><span class="line">        [ 2,  2,  2,  2],</span><br><span class="line">        [ 3,  3,  3,  3]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.data_ptr() == x.expand(3,4).data_ptr() # 内存共享</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<h2 id="torchrepeat_interleave"><a class="markdownIt-Anchor" href="#torchrepeat_interleave"></a> torch.repeat_interleave()</h2>
<p>将输入张量，按照指定维度重复指定次数，<strong>一个元素一个元素地重复</strong>（<code>tensor.repeat()</code>是一组一组重复）。<br />
返回的数组与输入数组维数相同，并且除了给定的维度dim，其他维度大小与输入数组相应维度大小相同。</p>
<p>参数</p>
<ul>
<li>input (Tensor)</li>
<li>repeats (Tensor or int)：元素的重复次数</li>
<li>dim=None (int, optional)：重复元素的维度；默认情况下将把给定的输入张量展平（flatten）为向量，然后将每个元素重复repeats次，并返回重复后的张量。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.arange(10).reshape(2, 5)</span><br><span class="line">tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; # repeat_interleave 是元素级别重复</span><br><span class="line">&gt;&gt;&gt; torch.repeat_interleave(a, 2, dim=0)</span><br><span class="line">tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9],</span><br><span class="line">        [5, 6, 7, 8, 9]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; # repeat 是整个 tensor 级别重复</span><br><span class="line">&gt;&gt;&gt; a.repeat(2, 2)</span><br><span class="line">tensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9],</span><br><span class="line">        [0, 1, 2, 3, 4, 0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])</span><br></pre></td></tr></table></figure>
<h2 id="广播"><a class="markdownIt-Anchor" href="#广播"></a> 广播</h2>
<p>在运行tensor的四则运算时，需要两个tensor维度相同。<br />
维度不同时，会有广播属性，即大小为1的维度会被复制（类似expand）为和另一个tensor相同的大小。</p>
<p>广播的条件为：</p>
<ul>
<li>数组拥有相同形状：无需广播。</li>
<li>后缘维度相同。</li>
<li>后缘维度不同，但是其中为1。</li>
</ul>
<p>若条件不满足，会抛出 “RuntimeError” 异常。</p>
<p>这时就需要使用<code>unsqueeze()</code>方法对张量之间进行扩维，使得后缘维度至少有一个为1。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.randn(3,2,4)</span><br><span class="line">&gt;&gt;&gt; b = torch.randn(3,4)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a+b</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a+b.unsqueeze(1)</span><br><span class="line">tensor([[[-1.6027, -0.9681, -0.2714,  0.4159],</span><br><span class="line">         [-1.5749,  4.0289,  0.6376, -1.0906]],</span><br><span class="line">        [[ 2.4601, -2.3880,  1.4608,  1.2630],</span><br><span class="line">         [ 0.5983, -2.4189,  0.7459,  1.2242]],</span><br><span class="line">        [[-0.6069, -1.6323,  0.9585,  0.9418],</span><br><span class="line">         [ 1.4031, -1.7707,  1.7068,  0.7386]]])</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.1.%20Torch.tensor%E5%88%87%E7%89%87%E5%92%8C%E7%B4%A2%E5%BC%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.1.%20Torch.tensor%E5%88%87%E7%89%87%E5%92%8C%E7%B4%A2%E5%BC%95/" class="post-title-link" itemprop="url">1.1 torch.tensor 切片和索引</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:32" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:32+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>209</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="索引"><a class="markdownIt-Anchor" href="#索引"></a> 索引</h2>
<ul>
<li>使用<code>[]</code>，不同维度之间使用<code>,</code>隔开。</li>
<li>和 numpy 类似</li>
<li>索引和切片后的tensor，和原始的tensor共享一个内存区域。可以使用<code>clone()</code>方法得到一个新的副本，进行后续修改操作。</li>
<li>支持 <strong>掩码</strong> 操作：传入一个相同大小的 boolean tensor，对应选择位置为 True；或者 8 位无符号整数 1。<br />
注意，返回的是一个一维 tensor！但是同样可以通过这个tensor进行后续操作。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor(range(6)).reshape(2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[0, 1, 2],</span><br><span class="line">        [3, 4, 5]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a&gt;3</span><br><span class="line">tensor([[False, False, False],</span><br><span class="line">        [False,  True,  True]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a[a&gt;3]</span><br><span class="line">tensor([4, 5])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a[a&gt;3]=100</span><br><span class="line">a</span><br><span class="line">tensor([[  0,   1,   2],</span><br><span class="line">        [  3, 100, 100]])</span><br></pre></td></tr></table></figure>
<h2 id="拼接"><a class="markdownIt-Anchor" href="#拼接"></a> 拼接</h2>
<p>与<code>numpy</code>中的<code>axis</code>不同 ，<code>tensor.tensor</code>轴的参数使用<code>dim</code>。下面例子中，torch中的<code>cat</code>方法即可以看作<code>np.concatnate</code>方法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; t1 = torch.cat([tensor, tensor, tensor], dim=1)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.2.%20Torch.tensor%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.2.%20Torch.tensor%E8%AE%A1%E7%AE%97/" class="post-title-link" itemprop="url">1.2 torch.tensor 计算</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:30" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:30+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>995</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="tensor内部"><a class="markdownIt-Anchor" href="#tensor内部"></a> tensor内部</h2>
<h3 id="平方根"><a class="markdownIt-Anchor" href="#平方根"></a> 平方根</h3>
<ul>
<li>内部方法：<code>tensor.sqrt()</code></li>
<li>torch函数形式：<code>torch.sqrt()</code><br />
这两个方法需要设置返回tensor来接收。</li>
<li>原地操作：<code>tensor.sqrt_</code></li>
</ul>
<h3 id="求和"><a class="markdownIt-Anchor" href="#求和"></a> 求和</h3>
<p><code>tensor.sum()</code>和<code>torch.sum()</code><br />
注意可以传入维度列表参数选定维度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[  0,   1],</span><br><span class="line">        [  2,   3],</span><br><span class="line">        [100, 100]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.sum(1)</span><br><span class="line">tensor([  1,   5, 200])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.sum(0)</span><br><span class="line">tensor([102, 104])</span><br></pre></td></tr></table></figure>
<h2 id="tensor间"><a class="markdownIt-Anchor" href="#tensor间"></a> tensor间</h2>
<p>注意两 tensor 的 dtype 一致。</p>
<h3 id="element-wise"><a class="markdownIt-Anchor" href="#element-wise"></a> element wise</h3>
<p><code>= - * /</code> 或者 <code>add()</code>,<code>sub()</code>,<code>mul()</code>,<code>dic()</code></p>
<p>如果张量中只有一个元素，可以使用<code>tensor.item()</code>得到这个单一元素。</p>
<h3 id="矩阵乘法点乘方法"><a class="markdownIt-Anchor" href="#矩阵乘法点乘方法"></a> 矩阵乘法/点乘方法</h3>
<p><code>tensor1 @ tensor2.T</code> ，是 <code>matmul</code> 的缩写。</p>
<p><code>tensor1.matmul(tensor2.T)</code> / <code>torch.matmul()</code></p>
<ul>
<li>如果两个tensor都是一维的，则为点乘运算，每个元素对应相乘求和。</li>
<li>如果两个都是二维的，那么就如同<code>torch.mm</code>。</li>
<li>如果第一个入参是一维的，第二个入参是二维的，则第一个参数增加一个一维，做<strong>矩阵乘法</strong>，结果然后去掉一维。</li>
<li>如果第一个入参是二维的，第二个是一维，则将第二个参数扩展一维，做矩阵乘法。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor([[1,2],[2,3]])</span><br><span class="line">&gt;&gt;&gt; b = torch.tensor([1,-1])</span><br><span class="line">&gt;&gt;&gt; a.matmul(b)</span><br><span class="line">tensor([-1, -1])</span><br></pre></td></tr></table></figure>
<ul>
<li>最后如果维度不同，涉及到batch乘法，<strong>（广播后）batch维度匹配</strong>：
<ul>
<li>两边shape的rank要相同，最后两维是用来做mm计算的</li>
<li>如果shape的rank不同，则可以通过广播使得batch维度相同。<br />
如<code>matmul(shape(10,4,2),shape(2,3))</code>则变为 <code>matmul(shape(10,4,2),shape(1,2,3))</code></li>
</ul>
</li>
</ul>
<p><code>tensor1.mm(tensor2.T)</code> / <code>torch.mm()</code></p>
<ul>
<li>不建议使用：仅提供矩阵相乘使用，使用范围较为狭窄。</li>
<li>如果维度超过二维，则会报错。RuntimeError: self must be a matrix</li>
</ul>
<p><code>tensor1.bmm(tensor2)</code> / <code>torch.bmm()</code><br />
batch-mm，不推荐，因为不支持自动broadcast。</p>
<h3 id="torcheinsum"><a class="markdownIt-Anchor" href="#torcheinsum"></a> torch.einsum()</h3>
<p>利用了爱因斯坦求和简介高效的表示方法，从而表示任何复杂的矩阵计算操作。</p>
<h4 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h4>
<p>缩并（Contraction）：对于更大维度的tensor乘法，需要决定tensor乘积结果需要沿着哪些维度求和。</p>
<p>现在考虑情况 <code>shape(3,3,3,2)</code> 和 <code>shape(2,4,2,3)</code> ，将最后两维度看作instance。<br />
我们想求，前后每两个instance的矩阵乘法结果。即答案为<code>shape(3,3,2,4,3,3)</code>，用<code>@</code>是无法求的，因为这不是batch乘法。<br />
同时即使维度相同，有时候也表示不同的batch/特征维度，还是两两相乘。</p>
<p>对于<code>einsum(A,B) =C</code>，首先将对应维度的下标（和维度无关）分为三类：</p>
<ul>
<li>ABC都共享：则下标对应的一系列元素需要作两两乘积（张量积）。</li>
<li>AB共享，C不共享：下表对应的元素需要做乘积求和（内积）。</li>
<li>AB共享，且维度相同，C只出现一次：按位置做乘法。</li>
</ul>
<h4 id="方法"><a class="markdownIt-Anchor" href="#方法"></a> 方法</h4>
<ul>
<li>输入的参数：
<ul>
<li><code>quation (str)</code>：输入tensor的下标和输出tensor的形状，以不同字母作区分。</li>
<li><code>operands (Tensor, [Tensor, …])</code>：数量及维度需与前面对应</li>
</ul>
</li>
<li>规则：
<ul>
<li>两个矩阵重复的index，会作乘积求和</li>
<li>省略的index会被加和</li>
<li>可以任意转换维度</li>
</ul>
</li>
<li>自由标和哑标：输入标记中仅出现一次的下标为自由标（free index），重复出现的下标为哑标（dummy/summation index），哑标对应的维度分量将被规约消去。</li>
<li>广播维度：省略号<code>...</code>表示维度的广播分量，例如，<code>i…j</code> 表示首末分量除外的维度需进行广播对齐</li>
<li><a target="_blank" rel="noopener" href="https://viatorsun.blog.csdn.net/article/details/122710515?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ELandingCtr%7ERate-1.queryctrv4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ELandingCtr%7ERate-1.queryctrv4&amp;utm_relevant_index=2">具体计算参考博客</a></li>
</ul>
<h4 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h4>
<p>一些乘法操作：</p>
<ul>
<li>Matrix Mult</li>
<li>Element-wise Mult/哈达玛积</li>
<li>Permutation</li>
<li>Dot product/Inner product：对应矩阵元素的积之和（形状相同）,sum(element_wise_dot)</li>
<li>Outer product/张量积: shape(m),shape(n)-&gt;shape(m,n)</li>
<li>Specific Summation</li>
<li>Batch Matrix Mult</li>
<li>教程：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XZ4y1Q7Ya?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1XZ4y1Q7Ya?spm_id_from=333.337.search-card.all.click</a></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 3,  4,  5]],</span><br><span class="line">        [[ 6,  7,  8],</span><br><span class="line">         [ 9, 10, 11]]])</span><br></pre></td></tr></table></figure>
<ul>
<li>permutation</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk-&gt;jik&quot;, a)</span><br><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 6,  7,  8]],</span><br><span class="line">        [[ 3,  4,  5],</span><br><span class="line">         [ 9, 10, 11]]])</span><br></pre></td></tr></table></figure>
<ul>
<li>column sum</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk-&gt;ik&quot;, a)</span><br><span class="line">tensor([[ 3,  5,  7],</span><br><span class="line">        [15, 17, 19]])</span><br></pre></td></tr></table></figure>
<ul>
<li>row sum</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk-&gt;ij&quot;, a)</span><br><span class="line">tensor([[ 3, 12],</span><br><span class="line">        [21, 30]])</span><br></pre></td></tr></table></figure>
<ul>
<li>Matrix Mult, <code>matmul()</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([2, 3, 4])</span><br><span class="line">&gt;&gt;&gt; b.shape</span><br><span class="line">torch.Size([2, 4, 3])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.einsum(&quot;bnk,bkl-&gt;bnl&quot;, a,b).shape</span><br><span class="line">torch.Size([2, 3, 3])</span><br></pre></td></tr></table></figure>
<ul>
<li>Dot product</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk,ijk-&gt;&quot;,a,a)</span><br><span class="line">tensor(506)</span><br></pre></td></tr></table></figure>
<ul>
<li>Element-wise Mult</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ij,ij-&gt;ij&quot;,a,a)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.3.%20Torch.tensor%E6%9E%81%E5%80%BC%E5%92%8C%E6%8E%92%E5%BA%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.3.%20Torch.tensor%E6%9E%81%E5%80%BC%E5%92%8C%E6%8E%92%E5%BA%8F/" class="post-title-link" itemprop="url">1.3 torch.tensor 极值和排序</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:30" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:30+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>194</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="argmax-argmin"><a class="markdownIt-Anchor" href="#argmax-argmin"></a> argmax() / argmin()</h2>
<p><code>tensor.argmax()</code>，传入维度<br />
<code>torch.argmax()</code>，传入tensor和维度<br />
返回的是每个维度对应的序号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor(</span><br><span class="line">            [[0, 1, 2],</span><br><span class="line">            [3, 10, 5],</span><br><span class="line">            [60, 7, 1]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.argmax(0)</span><br><span class="line">tensor([2, 1, 1])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.argmax(1)</span><br><span class="line">tensor([2, 1, 0])</span><br></pre></td></tr></table></figure>
<h2 id="max-min"><a class="markdownIt-Anchor" href="#max-min"></a> max() / min()</h2>
<p>返回位置和具体的值，返回的是元组形式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a.max(1)</span><br><span class="line">torch.return_types.max(</span><br><span class="line">    values=tensor([ 2, 10, 60]),</span><br><span class="line">    indices=tensor([2, 1, 0]))</span><br></pre></td></tr></table></figure>
<p>使用<code>a.max(1)[1]</code>来得到最大值结果。</p>
<h2 id="sort"><a class="markdownIt-Anchor" href="#sort"></a> sort()</h2>
<p><code>tensor.sort()</code> / <code>torch.sort()</code><br />
排序并且返回一个<code>namedtuple</code>:<code>(values, indices)</code>，即排序后的tensor，和tensor元素的原始位置。</p>
<p>参数：</p>
<ul>
<li><code>dim (int, optional)</code></li>
<li><code>descending = False (bool, optional)</code>：是否降序，默认为False（升序）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ 0,  1,  2],</span><br><span class="line">        [ 3, 10,  5],</span><br><span class="line">        [60,  7,  1]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.sort(0)</span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values=tensor([[ 0,  1,  1],</span><br><span class="line">                [ 3,  7,  2],</span><br><span class="line">                [60, 10,  5]]),</span><br><span class="line">indices=tensor([[0, 0, 2],</span><br><span class="line">                [1, 2, 0],</span><br><span class="line">                [2, 1, 1]]))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a = torch.ones(2,3)</span><br></pre></td></tr></table></figure>
<p>使用<code>a.sort(1)[0]</code>得到排序后结果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.%20Torch.tensor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.%20Torch.tensor/" class="post-title-link" itemprop="url">1. torch.tensor</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:29" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:29+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>6 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="tensors-介绍"><a class="markdownIt-Anchor" href="#tensors-介绍"></a> Tensors 介绍</h2>
<p>和<code>ndarray</code>类似，区别在于<code>tensor</code>可以在GPU或者其他硬件上运行。<br />
<code>tensor</code>和<code>ndarray</code>共享底层空间，所以不需要再次拷贝数据。</p>
<h2 id="tensor-属性"><a class="markdownIt-Anchor" href="#tensor-属性"></a> Tensor 属性</h2>
<ul>
<li><code>tensor.shape</code></li>
<li><code>tensor.dtype</code></li>
<li><code>tensor.device</code>：当前张量的设备名</li>
</ul>
<h2 id="tensor-构造方法"><a class="markdownIt-Anchor" href="#tensor-构造方法"></a> Tensor 构造方法</h2>
<h3 id="从数组创建"><a class="markdownIt-Anchor" href="#从数组创建"></a> 从数组创建</h3>
<ul>
<li><code>torch.tensor(arr)</code></li>
</ul>
<h3 id="从ndarray创建"><a class="markdownIt-Anchor" href="#从ndarray创建"></a> 从<code>ndarray</code>创建</h3>
<ul>
<li><code>torch.from_numpy(ndarray)</code>：从<code>ndarray</code>创建</li>
</ul>
<h3 id="从其他的tensor创建"><a class="markdownIt-Anchor" href="#从其他的tensor创建"></a> 从其他的tensor创建</h3>
<p>默认<code>shape</code>和<code>dtype</code>一致。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(3,3)</span><br><span class="line"></span><br><span class="line">torch.zeros_like(t)</span><br><span class="line">torch.ones_like(t)</span><br><span class="line">torch.rand_like(t)</span><br><span class="line">torch.randn_like(t)</span><br></pre></td></tr></table></figure>
<h3 id="随机值或者任意值填充"><a class="markdownIt-Anchor" href="#随机值或者任意值填充"></a> 随机值或者任意值填充</h3>
<p>指定张量的形状，返回给定形状的张量。<br />
注意：<br />
形状参数可以传入列表 <code>[1,2,3]</code>或者组<code>(1,2,3)</code>，也可以直接当做参数传入 <code>1,2,3</code>。<br />
同时直接以参数形式传入，可以在后面使用<code>dtype</code>参数设置类型。</p>
<ul>
<li><code>torch.rand()</code>生成的服从<code>[0，1)</code>上的<strong>均匀分布</strong>。</li>
<li><code>torch.randn()</code>生成的张量服从平均值为 0 ，标准差为 1 的<strong>正态分布</strong>。</li>
<li><code>torch.zeros()</code>生成全为 0 的张量，<code>ones()</code>全为 1 。</li>
<li><code>torch.eye(n)</code>生成 n*n 单位矩阵</li>
<li><code>torch.randint(0,10, (3,3))</code> 生成 <code>[0,10)</code> 均匀分布整数的矩阵。</li>
<li><code>torch.full(shape, fill_value)</code>生成一个尺寸为<code>shape</code>参数的<code>tensor</code>，元素全都哦是<code>fill_value</code>，数据类型为<code>torch.float</code>。</li>
</ul>
<h3 id="与numpy共享内存"><a class="markdownIt-Anchor" href="#与numpy共享内存"></a> 与Numpy共享内存</h3>
<p>在CPU中的张量可以与Numpy数组共享底层内存地址。<br />
同时，改变其中一个，同时会改变另一个数值。</p>
<ul>
<li><code>tensor.numpy()</code>：tensor转ndarray</li>
<li><code>torch.from_numpy(ndarray)</code>：ndarray转ndarray</li>
</ul>
<h2 id="设置存储设备"><a class="markdownIt-Anchor" href="#设置存储设备"></a> 设置存储设备</h2>
<p>CPU / GPU<br />
两个或者多个张量之间的运算，只有在<strong>相同的设备</strong>上才能进行。<br />
没有指定设备时，张量会被<strong>默认创建在CPU上</strong>。想转移 GPU 时，需要指定 目标GPU设备。<br />
将大型数组拷贝到GPU中需要耗费大量时间和资源。<br />
一般 GPU 设备以 <code>cuda:0</code>,<code>cuda:1</code>,… 指定。 查看显卡的信息可以使用<code>nvidia-smi</code>命令。</p>
<h3 id="设置设备"><a class="markdownIt-Anchor" href="#设置设备"></a> 设置设备</h3>
<h4 id="创建时设置"><a class="markdownIt-Anchor" href="#创建时设置"></a> 创建时设置</h4>
<p>在前面创建张量的方法中，可以使用<code>device</code>参数来选择存储位置。</p>
<h4 id="创建后转移"><a class="markdownIt-Anchor" href="#创建后转移"></a> 创建后转移</h4>
<ul>
<li>使用转移方法<code>.to</code>：<code>to(&quot;cpu&quot;)</code> / <code>to(&quot;cuda&quot;)</code> / <code>to(&quot;cuda:1&quot;)</code></li>
<li>直接使用 <code>cpu()</code>，<code>cuda()</code>方法。<code>cuda()</code> 方法可以传入目标设备编号参数。<br />
<strong>注意方法不会更改原始张量的存储设备，而是会返回一个新的在指定设备上的张量。</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.rand(2,2)</span><br><span class="line">&gt;&gt;&gt; a.device</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.to(&quot;cuda:1&quot;).device</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.cuda(2).device</span><br></pre></td></tr></table></figure>
<h3 id="常用方法"><a class="markdownIt-Anchor" href="#常用方法"></a> 常用方法</h3>
<p>开局指定一个设备，赋值给 <code>device</code> 参数，后面直接调用 <code>.to()</code> 方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">print(&#x27;Using &#123;&#125; device&#x27;.format(device))</span><br><span class="line"></span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(&#x27;cuda&#x27;)</span><br></pre></td></tr></table></figure>
<p>直接将 batch 中所有变量放到 <code>device</code> 中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_ids, masks, token_type_ids, labels = (i.to(device) for i in batch)</span><br></pre></td></tr></table></figure>
<p>即需要加一对 <code>(</code> <code>)</code>。</p>
<h2 id="维度相关的方法"><a class="markdownIt-Anchor" href="#维度相关的方法"></a> 维度相关的方法</h2>
<h3 id="查询一些属性"><a class="markdownIt-Anchor" href="#查询一些属性"></a> 查询一些属性</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(3,4)</span><br><span class="line">t.ndimension() # 维度数目：2</span><br><span class="line">t.nelement() # 张量元素总数：12</span><br><span class="line">t.size() / t.shape # 张量每个维度的大小：torch.Size([2, 3])</span><br><span class="line">t.size(0) # 张量维度0的大小：3</span><br></pre></td></tr></table></figure>
<h3 id="改变形状元素类型"><a class="markdownIt-Anchor" href="#改变形状元素类型"></a> 改变形状/元素类型</h3>
<h4 id="view"><a class="markdownIt-Anchor" href="#view"></a> view()</h4>
<p><code>t.view()</code>，可以传入<code>*shape</code>或者<code>dtype</code>来返回一个新的视图。<br />
该方法<strong>不改变底层数据</strong>，即地址相同，存储方式都是<strong>横向线性</strong>的。修改「view后张量」会改变「原来的张量」。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.ones(2,2)</span><br><span class="line">&gt;&gt;&gt; b = a.view(-1)</span><br><span class="line">tensor([1., 1., 1., 1.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; b[0] = 0</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">tensor([0., 1., 1., 1.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[0., 1.],</span><br><span class="line">        [1., 1.]])</span><br></pre></td></tr></table></figure>
<p>获取数据指针（地址）：<code>data_ptr()</code>方法。<br />
当用另一个变量得到<code>view()</code>的结果后，地址会改变，但是变量本身的地址不变。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a.data_ptr() == a.view(-1).data_ptr()</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.transpose(0,1).data_ptr()  == a.data_ptr()</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; b = a.view(-1)</span><br><span class="line">&gt;&gt;&gt; b.data_ptr() == a.data_ptr()</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p><code>view()</code>只是重新定义了访问张量的规则，使得取出的张量按照我们希望的形状展现。<br />
即只是重新定义下标与元素的对应关系的。换句话说，这种操作不进行数据拷贝和数据的改变，变的是元数据。这些操作是 <code>narrow()</code>，<code>view()</code>，<code>expand()</code>和<code>transpose()</code>。<br />
转置的tensor和原tensor的内存是共享的！<br />
当调用<code>contiguous()</code>时，会强制拷贝一份tensor。<br />
总之，如果想要新建一个新的tensor，就加上<code>contiguous()</code>吧，或者以后都加上吧。</p>
<h4 id="reshape"><a class="markdownIt-Anchor" href="#reshape"></a> reshape()</h4>
<p>当维度不兼容时，会加上<code>contiguous()</code>方法，这时采用直接调用<code>reshape()</code>方法，自动生成一个新的张量（等于<code>view()</code>+<code>contiguous()</code>）。<br />
区别是<code>view()</code>只能操作contiguous的tensor，且<code>view()</code>后的tensor和原tensor共享存储，<code>reshape()</code>对于是否contiuous的tensor都可以操作。</p>
<h2 id="contiguous"><a class="markdownIt-Anchor" href="#contiguous"></a> contiguous()</h2>
<p>由于pytorch的底层实现是C，也就是行优先存储。<br />
由最后输出的faltten后的结果可以看出存储的内容确实改变了，由此完全弄懂了为什么有的时候要contiguous。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;t = torch.arange(12).reshape(3,4)</span><br><span class="line">&gt;&gt;&gt;t</span><br><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br><span class="line">&gt;&gt;&gt;t.stride()</span><br><span class="line">(4, 1)</span><br><span class="line">&gt;&gt;&gt;t2 = t.transpose(0,1)</span><br><span class="line">&gt;&gt;&gt;t2</span><br><span class="line">tensor([[ 0,  4,  8],</span><br><span class="line">        [ 1,  5,  9],</span><br><span class="line">        [ 2,  6, 10],</span><br><span class="line">        [ 3,  7, 11]])</span><br><span class="line">&gt;&gt;&gt;t2.stride()</span><br><span class="line">(1, 4)</span><br><span class="line">&gt;&gt;&gt;t.data_ptr() == t2.data_ptr() # 底层数据是同一个一维数组</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt;t.is_contiguous(),t2.is_contiguous() # t连续，t2不连续</span><br><span class="line">(True, False)</span><br><span class="line">&gt;&gt;&gt;print(t1.flatten())</span><br><span class="line">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br><span class="line">&gt;&gt;&gt;t2 = t2.contiguous()</span><br><span class="line">&gt;&gt;&gt;print(t2.flatten())</span><br><span class="line">tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Numpy/13.%E6%95%B0%E7%BB%84%E7%9A%84%E5%89%AF%E6%9C%AC%E5%92%8C%E8%A7%86%E5%9B%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Numpy/13.%E6%95%B0%E7%BB%84%E7%9A%84%E5%89%AF%E6%9C%AC%E5%92%8C%E8%A7%86%E5%9B%BE/" class="post-title-link" itemprop="url">13.数组的副本和视图</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 16:42:16" itemprop="dateCreated datePublished" datetime="2021-09-22T16:42:16+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-Package/" itemprop="url" rel="index"><span itemprop="name">Data Analysis Package</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-Package/Numpy/" itemprop="url" rel="index"><span itemprop="name">Numpy</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>224</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="副本和视图"><a class="markdownIt-Anchor" href="#副本和视图"></a> 副本和视图</h2>
<h3 id="副本"><a class="markdownIt-Anchor" href="#副本"></a> 副本</h3>
<p>是一个数据的完整的拷贝，如果我们对副本进行修改，它不会影响到原始数据，物理内存不在同一位置。</p>
<ul>
<li><code>ndarray.copy()</code> 函数创建一个副本。 对副本数据进行修改，不会影响到原始数据，它们物理内存不在同一位置。</li>
</ul>
<h3 id="视图"><a class="markdownIt-Anchor" href="#视图"></a> 视图</h3>
<p>数据的一个别称或引用，通过该别称或引用亦便可访问、操作原有数据，但原有数据不会产生拷贝。<br />
如果我们对视图进行修改，它会影响到原始数据，物理内存在同一位置。<br />
- numpy 的<strong>切片操作</strong>返回原数据的视图。<br />
- 调用 ndarray 的 <code>view()</code> 函数产生一个视图。</p>
<ul>
<li><code>ndarray.view()</code> 方会创建一个新的数组对象，该方法创建的新数组的维数变化不会改变原始数据的维数。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Numpy/12.%E6%8E%92%E5%BA%8F%E3%80%81%E6%9D%A1%E4%BB%B6%E7%AD%9B%E9%80%89%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Numpy/12.%E6%8E%92%E5%BA%8F%E3%80%81%E6%9D%A1%E4%BB%B6%E7%AD%9B%E9%80%89%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">12.排序、条件筛选函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 15:56:48" itemprop="dateCreated datePublished" datetime="2021-09-22T15:56:48+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-Package/" itemprop="url" rel="index"><span itemprop="name">Data Analysis Package</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-Package/Numpy/" itemprop="url" rel="index"><span itemprop="name">Numpy</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>344</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>
<p><code>numpy.sort(a, axis, kind, order)</code>：返回输入数组的排序副本</p>
<ul>
<li><code>axis</code>：沿着它排序数组的轴，如果没有数组会被展开，沿着最后的轴排序。axis=0 按列排序，axis=1 按行排序。</li>
<li><code>kind</code>：默认为<code>'quicksort'</code>（快速排序），还可以选择归并排序<code>mergesort</code>和堆排序<code>heapsort</code>。</li>
<li><code>order</code>：如果数组包含字段，则是要排序的字段。</li>
</ul>
</li>
<li>
<p><code>numpy.argsort(a, axis=- 1, kind=None, order=None)</code>：返回的是数组值从小到大的索引值数组</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = np.array([3,  1,  2])  </span><br><span class="line">&gt;&gt;&gt; y = np.argsort(x)  </span><br><span class="line">&gt;&gt;&gt; y</span><br><span class="line">[1 2 0]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x[y]</span><br><span class="line">[1 2 3]</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><code>numpy.lexsort( (arr1, arr2, ..), axis=- 1 )</code>：多个序列进行排序。把它想象成对电子表格进行排序，每一列代表一个序列，排序时优先照顾靠后的列。</p>
</li>
<li>
<p><code>numpy.argmax(a, axis=None)</code> 和 <code>numpy.argmin(a, axis=None)</code>：分别沿给定轴返回最大和最小元素的索引。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = np.array([ [30,40,70],</span><br><span class="line">                   [80,20,10],</span><br><span class="line">                   [50,90,60] ])  </span><br><span class="line">&gt;&gt;&gt; np.argmax(a)</span><br><span class="line">7</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.argmax(a, axis = 0)  </span><br><span class="line">[1 2 0]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.argmax(a, axis = 1)  </span><br><span class="line">[2 0 1]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>numpy.nonzero(a)</code>：返回输入数组中非零元素的索引。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = np.array([[30,40,0],[0,20,10],[50,0,60]])  </span><br><span class="line">&gt;&gt;&gt; np.nonzero (a)</span><br><span class="line">(array([0, 0, 1, 1, 2, 2]), array([0, 1, 1, 2, 0, 2]))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>numpy.extract(condition, arr)</code>：根据某个条件从数组中抽取元素，返回满条件的元素。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; arr = np.arange(12).reshape((3, 4))</span><br><span class="line">&gt;&gt;&gt; arr</span><br><span class="line">array([[ 0,  1,  2,  3],</span><br><span class="line">       [ 4,  5,  6,  7],</span><br><span class="line">       [ 8,  9, 10, 11]])</span><br><span class="line">&gt;&gt;&gt; condition = np.mod(arr, 3)==0</span><br><span class="line">&gt;&gt;&gt; condition</span><br><span class="line">array([[ True, False, False,  True],</span><br><span class="line">       [False, False,  True, False],</span><br><span class="line">       [False,  True, False, False]])</span><br><span class="line">&gt;&gt;&gt; np.extract(condition, arr)</span><br><span class="line">array([0, 3, 6, 9])</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Numpy/11.%E6%95%B0%E7%BB%84%E7%BB%9F%E8%AE%A1%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Numpy/11.%E6%95%B0%E7%BB%84%E7%BB%9F%E8%AE%A1%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">11.数组统计函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 14:34:22" itemprop="dateCreated datePublished" datetime="2021-09-22T14:34:22+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-Package/" itemprop="url" rel="index"><span itemprop="name">Data Analysis Package</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-Package/Numpy/" itemprop="url" rel="index"><span itemprop="name">Numpy</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>393</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>NumPy 提供了很多统计函数，用于从数组中查找最小元素，最大元素，百分位标准差和方差等。</p>
<ul>
<li><code>numpy.amin(a, axis=None,)</code> ： 计算数组中的元素沿指定轴的最小值。<br />
<code>numpy.amax(a, axis=None,)</code>：计算数组中的元素沿指定轴的最大值。
<ul>
<li><code>axis</code>：不指定时返回数组中最小/大的一个元素。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = np.array( [ [3,7,5],</span><br><span class="line">                    [8,4,3], </span><br><span class="line">                    [2,4,9] ] ) </span><br><span class="line">&gt;&gt;&gt; np.amax(a)</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.amin(a,1)</span><br><span class="line">[3 3 2]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.amin(a,0)</span><br><span class="line">[2 4 3]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>numpy.ptp(a, axis=None, ..)</code>：计算数组中元素最大值与最小值的差 <strong>（最大值 - 最小值）</strong> 。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; np.array([[3,7,5],[8,4,3],[2,4,9]])  </span><br><span class="line">&gt;&gt;&gt; np.ptp(a)</span><br><span class="line">7</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.ptp(a, axis =  1)</span><br><span class="line">[[4 5 7]]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.ptp(a, axis =  0)</span><br><span class="line">[6 3 6]</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><code>numpy.percentile(a, q, axis=None)</code>：计算百分位数</p>
<ul>
<li><code>a</code>: 输入数组</li>
<li><code>q</code>: 要计算的百分位数，在 0 ~ 100 之间</li>
<li><code>axis</code>: 沿着它计算百分位数的轴</li>
</ul>
</li>
<li>
<p><code>numpy.median(a, axis=None)</code>：计算中位数</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = np.array([ [30,65,70],</span><br><span class="line">                   [80,95,10],</span><br><span class="line">                   [50,90,60] ])  </span><br><span class="line">&gt;&gt;&gt; np.median(a)</span><br><span class="line">65.0</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.median(a, axis =  0)</span><br><span class="line">[50. 90. 60.]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.median(a, axis =  1)</span><br><span class="line">[65. 80. 60.]</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><code>numpy.mean(a, axis=None)</code>：计算平均值<br />
算术平均值是沿轴的元素的总和除以元素的数量。</p>
</li>
<li>
<p><code>numpy.average(a, axis=None, weights=None, returned=False)</code>：在另一个数组中给出的各自的权重计算数组中元素的加权平均值</p>
<ul>
<li><code>axis</code>：该函数可以接受一个轴参数。 如果没有指定轴，则数组会被展开。</li>
<li><code>weights</code>：不指定权重时，相当于<code>np.mean()</code>函数。</li>
<li><code>returned</code>：在设置为True时，返回<code>(average, sum_of_weights)</code>元组。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = np.arange(6).reshape(3,2)  </span><br><span class="line">&gt;&gt;&gt; wt = np.array([3,5])  </span><br><span class="line">&gt;&gt;&gt; np.average(a, axis =  1, weights = wt)</span><br><span class="line">[0.625 2.625 4.625]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; np.average(a, axis =  1, weights = wt, returned =  True)</span><br><span class="line">(array([0.625, 2.625, 4.625]), array([8., 8., 8.]))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>np.std()</code>：标准差</li>
<li><code>np.var()</code>：方差</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/24/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/26/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">A foolish, slow learner.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">352</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YiandLi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YiandLi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yiliiiii" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yiliiiii" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">602k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">50:12</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
