<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":20,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A foolish, slow learner.">
<meta property="og:type" content="website">
<meta property="og:title" content="Yili">
<meta property="og:url" content="http://example.com/page/25/index.html">
<meta property="og:site_name" content="Yili">
<meta property="og:description" content="A foolish, slow learner.">
<meta property="og:locale">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/25/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Yili</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yili</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Suggest Google Chrome for better math Reading.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/3.3%20nn.LSTM&GRU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/3.3%20nn.LSTM&GRU/" class="post-title-link" itemprop="url">3.3 nn.RNN & LSTM & GRU</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:56:59" itemprop="dateCreated datePublished" datetime="2021-09-22T23:56:59-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>632</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>3 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="class-torchnnrnn"><a class="markdownIt-Anchor" href="#class-torchnnrnn"></a> CLASS torch.nn.RNN</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.RNN(*args, **kwargs)</span><br></pre></td></tr></table></figure>
<h2 id="初始化参数"><a class="markdownIt-Anchor" href="#初始化参数"></a> 初始化参数</h2>
<ul>
<li><code>input_size</code>：输入特征维度</li>
<li><code>hidden_size</code>：隐藏层维度</li>
<li><code>num_layers=1</code>：网络的层数，上层的输入是下层的输出，同时不同层间的参数是独立的。</li>
<li><code>nonlinearity='tanh'</code>：选择非线性激活函数，<code>'tanh'</code>或者<code>'relu'</code>。</li>
<li><code>bias=True</code>：对输入和隐状态作线性变换时，是否设置偏秩。</li>
<li><code>batch_first=False</code>：如果为<code>True</code>，则输入输出tensor尺寸为<code>(batch, seq, feature)</code>，否则为<code>(seq, batch, feature)</code>。<br />
Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details.</li>
<li><code>dropout=0</code>：如果不为0，则给每层输出层加一个dropout（除了最后一层）。</li>
<li><code>bidirectional=False</code></li>
</ul>
<p>N = batch size<br />
L = sequence length<br />
D = 2 if bidirectional=True otherwise 1<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = input_size<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = hidden_size</p>
<h2 id="inputs-input-h_0"><a class="markdownIt-Anchor" href="#inputs-input-h_0"></a> Inputs: input, h_0</h2>
<ul>
<li><code>input</code>：
<ul>
<li>tensor，如果<code>batch_first=False</code>，其形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L,N,H_{in})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ；否则形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,L,H_{in})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
<li>PackedSequence，<code>pack_padded_sequence()</code>或<code>pack_sequence()</code>的返回值。</li>
</ul>
</li>
<li><code>h_0=0</code>：初始隐藏层状态，形状 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>∗</mo><mtext>num_layers</mtext><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D * \text{num\_layers} , N, H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">num_layers</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ，默认为0。</li>
</ul>
<h2 id="outputs-output-h_n"><a class="markdownIt-Anchor" href="#outputs-output-h_n"></a> Outputs: output, h_n</h2>
<ul>
<li><code>output</code>：最后一层的输出特征
<ul>
<li>如果<code>batch_first=False</code>，则其形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo>∗</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L,N,D*H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ；否则为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>D</mi><mo>∗</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,L,D*H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。<br />
对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">D=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> 时候的输出，可以使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">.</mi><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">.view(-1,2,H_{output})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">i</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 方式得到两个前后输出向量。</li>
<li>如果输入的是 <code>PackedSequence，</code>，则输出也是 <code>PackedSequence</code>。</li>
</ul>
</li>
<li><code>h_n</code>：从下到上每一层的最终隐状态；形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>∗</mo><mtext>num_layers</mtext><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D*\text{num\_layers},N,H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">num_layers</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
</ul>
<h1 id="class-torchnnlstm"><a class="markdownIt-Anchor" href="#class-torchnnlstm"></a> CLASS torch.nn.LSTM</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LSTM(*args, **kwargs)</span><br></pre></td></tr></table></figure>
<h2 id="初始化参数-2"><a class="markdownIt-Anchor" href="#初始化参数-2"></a> 初始化参数</h2>
<ul>
<li><code>input_size</code></li>
<li><code>hidden_size</code></li>
<li><code>num_layers=1</code></li>
<li><code>bias=True</code></li>
<li><code>batch_first=False</code></li>
<li><code>dropout=0</code></li>
<li><code>bidirectional=False</code></li>
<li><code>proj_size=0</code>：如果大于0，则隐藏层状态会被投影到相应的维度。</li>
</ul>
<h2 id="inputs-input-h_0-c_0"><a class="markdownIt-Anchor" href="#inputs-input-h_0-c_0"></a> Inputs: input, (h_0, c_0)</h2>
<ul>
<li><code>input</code>：输入序列
<ul>
<li>tensor，如果<code>batch_first=False</code>，其形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L,N,H_{in})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ；否则形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,L,H_{in})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
<li>PackedSequence，<code>pack_padded_sequence()</code>或<code>pack_sequence()</code>的返回值。</li>
</ul>
</li>
<li><code>h_0=0</code>：初始隐藏层状态，形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>∗</mo><mtext>num_layers</mtext><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D*\text{num\_layers},N,H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">num_layers</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
<li><code>c_0=0</code>：初始细胞状态，形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>∗</mo><mtext>num_layers</mtext><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>c</mi><mi>e</mi><mi>l</mi><mi>l</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D*\text{num\_layers},N,H_{cell})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">num_layers</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mi>N</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext> batch size </mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mi>L</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext> sequence length </mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mi>D</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mn>2</mn><mtext> if bidirectional=True otherwise </mtext><mn>1</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext> input_size </mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>H</mi><mrow><mi>c</mi><mi>e</mi><mi>l</mi><mi>l</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext> hidden_size </mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext> proj_size if proj_size </mtext><mo>&gt;</mo><mn>0</mn><mtext> otherwise hidden_size </mtext></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
N &amp;=\text { batch size } \\
L &amp;=\text { sequence length } \\
D &amp;=2 \text { if bidirectional=True otherwise } 1 \\
H_{i n} &amp;=\text { input\_size } \\
H_{c e l l} &amp;=\text { hidden\_size } \\
H_{o u t} &amp;=\text { proj\_size if proj\_size }&gt;0 \text { otherwise hidden\_size } \end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:9.000000000000002em;vertical-align:-4.250000000000001em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.750000000000001em;"><span style="top:-6.910000000000001em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">L</span></span></span><span style="top:-3.9099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span><span style="top:-2.4099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-0.9099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:0.5900000000000007em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:4.250000000000001em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.750000000000001em;"><span style="top:-6.910000000000001em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord"> batch size </span></span></span></span><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord"> sequence length </span></span></span></span><span style="top:-3.9099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">2</span><span class="mord text"><span class="mord"> if bidirectional=True otherwise </span></span><span class="mord">1</span></span></span><span style="top:-2.4099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord"> input_size </span></span></span></span><span style="top:-0.9099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord"> hidden_size </span></span></span></span><span style="top:0.5900000000000007em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord"> proj_size if proj_size </span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span><span class="mord text"><span class="mord"> otherwise hidden_size </span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:4.250000000000001em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<h2 id="outputs-output-h_n-c_n"><a class="markdownIt-Anchor" href="#outputs-output-h_n-c_n"></a> Outputs: output, (h_n, c_n)</h2>
<ul>
<li><code>output</code>：最后一层的输出特征
<ul>
<li>如果<code>batch_first=False</code>，则其形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo>∗</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L,N,D*H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ；否则为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>D</mi><mo>∗</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,L,D*H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。<br />
对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">D=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> 时候的输出，可以使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">.</mi><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">.view(-1,2,H_{output})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">i</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 方式得到两个前后输出向量。</li>
<li>如果输入的是 <code>PackedSequence，</code>，则输出也是 <code>PackedSequence</code>。</li>
</ul>
</li>
<li><code>h_n</code>：每一层+方向上的最终隐藏层状态；形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>∗</mo><mtext>num_layers</mtext><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D*\text{num\_layers}, N, H_{out})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">num_layers</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
<li><code>c_n</code>：每一层+方向上的最终细胞状态；形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>∗</mo><mtext>num_layers</mtext><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>c</mi><mi>e</mi><mi>l</mi><mi>l</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D*\text{num\_layers}, N, H_{cell})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">num_layers</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/3.2%20rnn.pad&pack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/3.2%20rnn.pad&pack/" class="post-title-link" itemprop="url">3.2 rnn.pad&pack</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:48:59" itemprop="dateCreated datePublished" datetime="2021-09-22T23:48:59-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>894</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="pad-pack"><a class="markdownIt-Anchor" href="#pad-pack"></a> pad, pack</h2>
<p>正常情况下传入 RNN 的 tensor 是被 pad 后的，即所有句子全都等长。<br />
这时，我们不希望模型计算最后的<code>[pad]</code>token。因为这样会导致训练时间变长，模型效果下降。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/09/22/Coding/Pytorch/3.2%20rnn.pad&pack/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/3.1%20nn.Embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/3.1%20nn.Embedding/" class="post-title-link" itemprop="url">3.1 nn.Embedding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:47:59" itemprop="dateCreated datePublished" datetime="2021-09-22T23:47:59-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>575</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>3 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Embedding( num_embeddings, embedding_dim, padding_idx=None,</span><br><span class="line">                    max_norm=None,  norm_type=2.0, scale_grad_by_freq=False,</span><br><span class="line">                    sparse=False,  _weight=None)</span><br></pre></td></tr></table></figure>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/09/22/Coding/Pytorch/3.1%20nn.Embedding/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/3.%20Torch%20%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/3.%20Torch%20%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">3. 构造模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:46:59" itemprop="dateCreated datePublished" datetime="2021-09-22T23:46:59-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>16 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>知乎教程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/340453841">https://zhuanlan.zhihu.com/p/340453841</a><br />
CSDN：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_23981335/article/details/103683737">https://blog.csdn.net/qq_23981335/article/details/103683737</a><br />
<code>torch.nn</code>提供了所有的神经网络层。所有的模型和层都是<code>nn.Module</code>的子类。<br />
torch中的神经网络是Module模块组成的Module模块。</p>
<h2 id="定义训练设备"><a class="markdownIt-Anchor" href="#定义训练设备"></a> 定义训练设备</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">print(&#x27;Using &#123;&#125; device&#x27;.format(device))</span><br></pre></td></tr></table></figure>
<h2 id="模型的参数"><a class="markdownIt-Anchor" href="#模型的参数"></a> 模型的参数</h2>
<ul>
<li>
<p>神经网络内部的许多层都是参数化的，比如权重w和偏置b。<br />
<code>nn.Module</code>对象自动跟踪所有内部的参数，可以通过模型的<code>parameters()</code>和<code>named_parameter()</code>方法来获取模型的所有参数。</p>
</li>
<li>
<p><code>nn.Parameter</code>：是<code>torch.tensor</code>的子类，默认<code>requires_grad=True</code>，并且绑定到了该模型的<code>.parameters()</code>字典中。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Mnist_Logistic(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Mnist_Logistic, self).__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(10))</span><br></pre></td></tr></table></figure>
<p>和普通的<code>tensor(.. ,requires_grad=True)</code>不同的在于，<code>Parameter</code>对象是Module的一部分，即如果只是Tensor的话，这个张量虽然参与了计算，但是需要单独更新：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights -= weights.grad * lr</span><br><span class="line">bias -= bias.grad * lr</span><br></pre></td></tr></table></figure>
<p>但是，如果使用<code>Parameter()</code>实例进行包装，则直接对<code>model.parameters()</code>字典进行更新即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): p -= p.grad * lr</span><br></pre></td></tr></table></figure>
<h2 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h2>
<p><code>nn.Module</code>是PyTorch体系下所有神经网络模块的基类。<br />
<img src="https://pic2.zhimg.com/80/v2-bacaa829d1fd6c3dca94c3311f5383d1_1440w.jpg" alt="" /></p>
<p>通过继承<code>nn.Module</code>来定义神经网络，并且通过其<code>_init__()</code>方法初始化神经网络中的<code>module</code>，<code>parameter</code>和<code>buffer</code>。<br />
每一个<code>nn.Module</code>子类，即模型，都需要实现<code>forward()</code>方法来操作输入数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line">#将模型放入设备中</span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>模型进行前向传播时，除了<code>forward()</code>方法之外，还会有一些底层的运算。<br />
所以在得到输出时，不要直接调用模型的<code>forward()</code>方法，而是使用<code>output = model(input)</code>的形式。</li>
</ul>
<h3 id="__dict__-实例变量"><a class="markdownIt-Anchor" href="#__dict__-实例变量"></a> <code>__dict__</code> 实例变量</h3>
<p><code>model.__dict__</code>保存了模型的所有实例变量的键值对。</p>
<h3 id="属性添加"><a class="markdownIt-Anchor" href="#属性添加"></a> 属性添加</h3>
<ul>
<li><code>add_module(name, module)</code>：增加子神经网络模块，更新 <code>self._modules</code>。<br />
初始化类型为<code>nn.Module</code>。</li>
<li><code>register_parameter(name, param)</code>：增加通过BP更新的 parameters ，更新 <code>self._parameters</code>。<br />
初始化类型为<code>nn.Parameter</code>。</li>
<li><code>register_buffer(name, tensor, persistent=True)</code>：增加不通过 BP 更新的 buffer，更新 <code>self._buffers</code><br />
初始化类型为<code>torch.Tensor</code>或者 <code>None</code>。<br />
默认<code>persistent=True</code>时，会将该buffer添加到模型的<code>state_dict</code>中。<br />
当<code>persistent=False</code>时，会更新到<code>self._non_persistent_buffers_set</code>中。</li>
</ul>
<p><strong>这三个方法中的name属性需要非空并且不包含<code>'.'</code>。</strong><br />
这 3 个函数都会:</p>
<ul>
<li>先检查 self.<strong>dict</strong> 中是否包含对应的属性字典以确保  <code>nn.Module</code> 被正确初始化。</li>
<li>然后检查属性的 <code>name</code> 是否合法</li>
<li>同时还会检查他们是否已经存在于要修改的属性字典中</li>
</ul>
<br>
<p>不过更常见的是使用<code>self.xxx = xxx</code>添加<code>module</code>和<code>parameter</code>。这种方式本质上会调用 <code>nn.Module</code> 重载的函数<code>__setattr__</code>。<br />
在调用在增加 <code>self._parameters</code>，<code>self._modules</code> 的时候，会预先调用 <code>remove_from</code> 函数从其余私有属性中删除对应的 name。<br />
这说明 <strong><code>self.dict</code>，<code>self._buffers</code>，<code>self._parameters</code>，<code>self._modules</code> 中的属性名是互斥的</strong> 。</p>
<ul>
<li>buffer是内存中的一个常量，在后向传播的过程中<strong>不会</strong>被更新，在模型保存和加载时，可以被写入和读出。<br />
例子：批次归一化层中的平均值和方差。<br />
对于buffer而言：<code>register_buffer()</code>用于<strong>新添加</strong>Buffer：<code>self.xxx = xxx</code>只能将<code>self._buffers</code>中已有的 buffer 重新赋值为 <code>None</code> 或者 <code>tensor</code>。</li>
</ul>
<blockquote>
<p><code>self.xxxx = torch.Tensor()</code> 是一种不被推荐的行为，因为这样新增的 attribute 既不属于 <code>self._parameters</code>，也不属于 <code>self._buffers</code>，而会被视为普通的 attribute ，在将模块进行状态转换的时候，<code>self.xxxx</code> 会被遗漏进而导致 device 或者 type 不一样的 bug。</p>
</blockquote>
<h3 id="属性删除"><a class="markdownIt-Anchor" href="#属性删除"></a> 属性删除</h3>
<p>属性的删除通过重载的<code>__delattr__(name)</code> 来实现。</p>
<h2 id="快速构造网络"><a class="markdownIt-Anchor" href="#快速构造网络"></a> 快速构造网络</h2>
<p><code>nn.Sequential</code>是一个module容器，输入输入会按照顺序依次经过容器内所有的moduel。<br />
里面可以放模型，也可以直接放置层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(20, 10)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(3,28,28)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure>
<h2 id="重新初始化-模型参数"><a class="markdownIt-Anchor" href="#重新初始化-模型参数"></a> 重新初始化 模型参数</h2>
<p>Linear层参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/tangjunjun/p/13731276.html">https://www.cnblogs.com/tangjunjun/p/13731276.html</a><br />
<code>layer.weight.data = target_tensor</code> 无效，这里选择<code>copy_()</code>方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier.weight[0].data.copy_(id_to_emb[i])   # 使用 &#x27;=&#x27; 赋值方法无效，遂用 `copy_()`</span><br></pre></td></tr></table></figure>
<h2 id="模型的方法"><a class="markdownIt-Anchor" href="#模型的方法"></a> 模型的方法</h2>
<h3 id="获取模型基本信息"><a class="markdownIt-Anchor" href="#获取模型基本信息"></a> 获取模型基本信息</h3>
<ul>
<li><code>__get_name()</code>：获取类名</li>
<li><code>__repr__()</code>：输出该Module中所有SubModule的信息并且one item per line</li>
<li><code>__dir__()</code>：<br />
输出该Module中包含的所有<code>self.__class__</code>、<code>self.__dict__.keys()</code>、<code>self._parameters.keys()</code>、<code>self._modules.keys()</code>以及<code>self._buffers.keys()</code>，并且会通过<code>key for key in keys if not key[0].isdigit()</code>来消除不合法的Python变量名称的属性。</li>
<li><code>__getattr__(self, name)</code>：获取给定name的Module类中的成员，返回一个具体的module类。</li>
<li><code>__setattr__(self, name, value)</code>：设置属性</li>
</ul>
<h3 id="获取内部层和参数"><a class="markdownIt-Anchor" href="#获取内部层和参数"></a> 获取内部层和参数</h3>
<ul>
<li><code>modules()</code>：返回一个包含 当前模型 所有模块的<strong>迭代器</strong>(母模型+子模型)。重名的模块只被返回一次(children()也是)。<br />
如何理解重名：地址不同的，即不是新构建的，下面的submoudle1和submoudle2虽然结构一致，但是是新构建的，参数不同，所以不是重名的。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        submodule = nn.Conv2d(10, 20, 4)</span><br><span class="line">        self.submodule1 = nn.Conv2d(10, 10, 4)</span><br><span class="line">        self.submodule2 = nn.Conv2d(10, 10, 4)</span><br><span class="line">        # 统一模块不同名称</span><br><span class="line">        self.add_module(&quot;conv&quot;, submodule)</span><br><span class="line">        self.add_module(&quot;conv1&quot;, submodule)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">modules = Model().modules()</span><br><span class="line">for _, i in enumerate(modules):</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">Model(</span><br><span class="line">  (submodule1): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (submodule2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (conv1): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">)</span><br><span class="line">Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><code>children()</code>：返回子模型的迭代器，即上面的部分去除母模型。</p>
</li>
<li>
<p><code>parameters()</code>：返回模型的参数（张量形式）迭代器，可以看作<code>named_parameters()</code>返回元组的参数部分。</p>
</li>
<li>
<p><code>buffers()</code>：调用 <code>self.named_buffers()</code> 并返回模型<strong>缓存</strong>。</p>
</li>
<li>
<p><code>named_buffers()</code>：返回 <code>self._buffers</code> 中的 <code>name</code> 和 <code>buffer</code> 元组。</p>
</li>
<li>
<p><code>named_children()</code></p>
</li>
<li>
<p><code>named_modules(memo=None, prefix='')</code></p>
</li>
<li>
<p><code>named_parameters()</code>：返回 <code>self._parameters</code>中的<code>name</code>和<code>parameter</code>元组。</p>
</li>
</ul>
<p>上述方法都不会返回重复的对象。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">============================================================</span><br><span class="line">&gt;&gt;&gt; Model().modules()</span><br><span class="line"></span><br><span class="line">Model(</span><br><span class="line">  (submodule1): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (submodule2): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (conv1): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">)</span><br><span class="line">Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; Model().named_modules()</span><br><span class="line"></span><br><span class="line">(&#x27;&#x27;, Model(</span><br><span class="line">  (submodule1): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (submodule2): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">  (conv1): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">))</span><br><span class="line">(&#x27;submodule1&#x27;, Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1)))</span><br><span class="line">(&#x27;submodule2&#x27;, Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1)))</span><br><span class="line">(&#x27;conv&#x27;, Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1)))</span><br><span class="line">============================================================</span><br><span class="line">&gt;&gt;&gt; Model().children()</span><br><span class="line"></span><br><span class="line">Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line">Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; Model().named_children()</span><br><span class="line"></span><br><span class="line">(&#x27;submodule1&#x27;, Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1)))</span><br><span class="line">(&#x27;submodule2&#x27;, Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1)))</span><br><span class="line">(&#x27;conv&#x27;, Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1)))</span><br><span class="line">============================================================</span><br><span class="line">&gt;&gt;&gt; Model().parameters()</span><br><span class="line"></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor( .... , requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor( .... , requires_grad=True)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; Model().named_parameters():</span><br><span class="line"></span><br><span class="line">(&#x27;submodule1.weight&#x27;, Parameter containing:</span><br><span class="line">tensor( .... , requires_grad=True)</span><br><span class="line">(&#x27;...&#x27;, tensor(..., ...))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以看到，返回的都是一个迭代器对象。而带<code>named-</code>的前缀方法的迭代对象都是一个元组，每个元组对象有两个元素，名称和具体的Parameter对象。而我们在分布添加到optimizer中的，就是这个具体的Parameter对象。</p>
</blockquote>
<h3 id="训练状态的转换"><a class="markdownIt-Anchor" href="#训练状态的转换"></a> 训练状态的转换</h3>
<p>nn.Module 通过 <code>self.training</code> 来区分训练和测试两种状态，使得模块可以在训练和测试时有不同的 <code>forward</code> 行为。<br />
nn.Module 通过 <code>self.train()</code> 和 <code>self.eval()</code> 来修改训练和测试状态，其中 <code>self.eval</code> 直接调用了 <code>self.train(False)</code>，而 <code>self.train()</code> 会修改 <code>self.training</code> 并通过 <code>self.children()</code> 来调整所有子模块的状态。</p>
<ul>
<li><code>train((mode=True)</code> ：只对<code>Dropout</code>和<code>BatchNorm</code>有效。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def train(self: T, mode: bool = True) -&gt; T:</span><br><span class="line">    self.training = mode</span><br><span class="line">    for module in self.children():</span><br><span class="line">        module.train(mode)</span><br><span class="line">    return self</span><br></pre></td></tr></table></figure>
<ul>
<li><code>eval()</code> == <code>train(False)</code>：只对<code>Dropout</code>和<code>BatchNorm</code>有效。</li>
</ul>
<h3 id="梯度的处理"><a class="markdownIt-Anchor" href="#梯度的处理"></a> 梯度的处理</h3>
<p>默认情况下，<code>parameter.require_grad()=True</code>，可以使用<code>requires_grad_()</code>方法和<code>zero_grad()</code>方法来设置梯度。<br />
源码来看， <code>requires_grad_(self: T, requires_grad: bool)</code>会遍历Module内的所有Parameter，并且对其设置<code>requires_grad_(requires_grad:bool)</code>方法。<br />
而<code>zero_grad()</code>方法最后是对<code>self.parameters()</code>中的每个元素使用了<code>.zero_()</code>方法进行清空。</p>
<h3 id="__applyfn与applyfn方法"><a class="markdownIt-Anchor" href="#__applyfn与applyfn方法"></a> <code>__apply(fn)</code>与<code>apply(fn)</code>方法</h3>
<p><code>nn.Module</code> 实现了如下 8 个常用<strong>函数</strong>将模块转变成不同的<strong>数据类型</strong>、并且转移到<strong>CPU/ GPU</strong>上。</p>
<ol>
<li><code>CPU</code>：将所有 parameters 和 buffer 转移到 CPU 上</li>
<li><code>type</code>：将所有 parameters 和 buffer 转变成另一个类型</li>
<li><code>CUDA</code>：将所有 parameters 和 buffer 转移到 GPU 上</li>
<li><code>float</code>：将所有浮点类型的 parameters 和 buffer 转变成 float32 类型</li>
<li><code>double</code>：将所有浮点类型的 parameters 和 buffer 转变成 double 类型</li>
<li><code>half</code>：将所有浮点类型的 parameters 和 buffer 转变成 float16 类型</li>
<li><code>bfloat16</code>：将所有浮点类型的 parameters 和 buffer 转变成 bfloat16 类型</li>
<li><code>to</code>：移动模块或/和改变模块的类型<br></li>
</ol>
<p>这些转换参数的数据类型(dtype)，转移数据位置(cpu/gpu)等操作最终都是通过<code>self._apply(function)</code>来实现的。function 一般是 lambda 表达式或其他自定义函数。</p>
<ul>
<li><code>self._apply(function)</code>函数做了三件事：
<ol>
<li>对<code>self.children()</code>中的子module进行递归调用：<code>module._apply(fn)</code>，最后处理到到下面两步。</li>
<li>对<code>self._parameters</code>中的<code>param.data</code>及其<code>param.gradient</code>通过 function 进行处理：<br />
<code>param.data=fn(param)</code>和<code>param.grad.data=fn(param.grad)</code>。<br />
在更改这两个值时，会使用<code>compute_should_use_set_data( param, fn(param) )</code>函数来判断是否原地修改。</li>
<li>对<code>self._buffers</code>中的 buffer 逐个通过 function 来进行处理：<code>self._buffers[key]=fn(buf)</code></li>
</ol>
</li>
<li><code>self.apply(fn)</code>：只是简单地递归调用了 <code>self.children()</code> 去处理自己以及子模块：<code>module.apply(fn)</code></li>
<li>应用：常用<code>apply</code>方法和自定义的<code>init_weights(moudle)</code>方法重新初始化参数：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@torch.no_grad()</span><br><span class="line">def init_weights(m):</span><br><span class="line">    print(m)</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        m.weight.fill_(1.0)</span><br><span class="line">        print(m.weight)</span><br><span class="line">s</span><br><span class="line">net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure>
<h3 id="自定义模型权重"><a class="markdownIt-Anchor" href="#自定义模型权重"></a> 自定义模型权重</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/dss_dssssd/article/details/83990511">https://blog.csdn.net/dss_dssssd/article/details/83990511</a></p>
<h2 id="模块的保存和加载"><a class="markdownIt-Anchor" href="#模块的保存和加载"></a> 模块的保存和加载</h2>
<p>PyTorch 集成了 Python 自带的<code>pickle</code>包对模块和tensor进行「序列化」和「反序列化」。<br />
本质上是把tensor的信息，包括数据类型和存储位置，以及数据等转化为字符串。随后将这些字符串通过 文件IO操作进行存储，这个过程可逆。</p>
<h3 id="保存整个模型"><a class="markdownIt-Anchor" href="#保存整个模型"></a> 保存整个模型</h3>
<p>和上面的方法一样，不同的是，我们住需要将原本的状态字典参数换成换成整个模型<code>model</code>即可。而加载时，则无需实例化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 保存</span><br><span class="line">torch.save(model, &#x27;model.pth&#x27;)</span><br><span class="line"># 加载</span><br><span class="line">torch.load(&#x27;model.pth&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li>缺点：PyTorch 的模块实现依赖于具体的 PyTorch 版本，所以版本不同时，载入整个模型可能会报错。</li>
</ul>
<h3 id="只保存权重参数"><a class="markdownIt-Anchor" href="#只保存权重参数"></a> 只保存权重参数</h3>
<p>PyTorch模型的参数被保存在一个状态字典<code>state_dict</code>中，这个字典保存了所有参数的名称和tensor值。<br />
我们可以通过<code>torch.save</code>方法来保存这个字典。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), &#x27;model_weights.pth&#x27;, pickle_module=pickle, pickle_protocol=2)</span><br></pre></td></tr></table></figure>
<p>参数<code>pickle</code>是默认的，传入的是序列化的库；<code>pickle_protocol</code>是pickle协议，一共有5个可选版本。</p>
<p>为了加载模型的权重参数，首先要创建一个相同的模型实例，然后调用其<code>load_state_dict()</code>方法加载参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ckpt = torch.load(&#x27;model_weights.pth&#x27;, map_location=None)</span><br><span class="line">model.load_state_dict(ckpt)</span><br></pre></td></tr></table></figure>
<p>参数<code>map_location</code>是tensor存储位置的映射。<br />
根据<code>save()</code>调用时候模型存储位置不同，该参数的选择也不同：</p>
<ol>
<li>如果存储时，模型在保存在CPU上，则直接默认即可。</li>
<li>如果存储时，模型保存在GPU上，则加载时程序默认是先将模型加载到CPU上，然后转移到保存编号（“cuda:0” …）的GPU上。<br />
这时，如果加载的设备没有GPU，或者GPU的设备编号不支持（单卡设备加载了&quot;cuda:1&quot;的模型），则会报错。<br />
解决方法是：设置<code>map_location='cpu'</code>，这样就先将模型加载到CPU上，后续调用<code>to()</code>进行设备转移。</li>
</ol>
<h3 id="优化器的保存"><a class="markdownIt-Anchor" href="#优化器的保存"></a> 优化器的保存</h3>
<p>训练过程中，不仅需要保存模型，我们也要保存 optimizer，支持继续训练。</p>
<p>optimizer 本身也带有状态字典 <code>state_dict</code> ，存储了当前学习率，梯度的指数移动平均等。通过调用<code>state_dict()</code>方法和<code>load_state_dict()</code>方法，可以让optimizer对象输出和载入相关的状态字典。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 信息</span><br><span class="line">save_info =  &#123;</span><br><span class="line">  &quot;iter_num&quot; : iter_name ,</span><br><span class="line">  &quot;optimizer&quot; : optimizer.state_dict() ,</span><br><span class="line">  &quot;model&quot; : model.state_dict() ,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 保存信息</span><br><span class="line">torch.save(save_info, save_path)</span><br><span class="line"></span><br><span class="line"># 先初始化</span><br><span class="line">optimizer = ...</span><br><span class="line">model = ...</span><br><span class="line"></span><br><span class="line"># 载入信息</span><br><span class="line">save_info = torch.load(save_path)</span><br><span class="line">optimizer.load_state_dict(save_info[&quot;optimizer&quot;])</span><br><span class="line">model.load_state_dict(save_info[&quot;model&quot;])</span><br></pre></td></tr></table></figure>
<h2 id="hook"><a class="markdownIt-Anchor" href="#hook"></a> Hook</h2>
<p>用于不改变网络，获取运行过程中的信息，比如参数和梯度值等。从而诊断网络的问题，分析可行性。</p>
<h2 id="直接输出模型参数量"><a class="markdownIt-Anchor" href="#直接输出模型参数量"></a> 直接输出模型参数量</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def print_variables(model, pattern=&quot;&quot;, log=True):</span><br><span class="line"># 支持 re 使用 pattern 选择</span><br><span class="line">    flags = []</span><br><span class="line"></span><br><span class="line">    for (name, var) in model.named_parameters():</span><br><span class="line">        if re.search(pattern, name):</span><br><span class="line">            flags.append(True)</span><br><span class="line">        else:</span><br><span class="line">            flags.append(False)</span><br><span class="line"></span><br><span class="line">    weights = &#123;v[0]: v[1] for v in model.named_parameters()&#125;</span><br><span class="line">    total_size = 0</span><br><span class="line"></span><br><span class="line">    for name in sorted(list(weights)):</span><br><span class="line">        if re.search(pattern, name):</span><br><span class="line">            v = weights[name]</span><br><span class="line">            total_size += v.nelement()</span><br><span class="line"></span><br><span class="line">            if log:</span><br><span class="line">                print(&quot;%s %s&quot; % (name.ljust(60), str(list(v.shape)).rjust(15)))</span><br><span class="line"></span><br><span class="line">    if log:</span><br><span class="line">        print(&quot;Total trainable variables size: %d&quot; % total_size)</span><br><span class="line"></span><br><span class="line">    return flags</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/2.%20Datasets%E5%92%8CDataloaders/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/2.%20Datasets%E5%92%8CDataloaders/" class="post-title-link" itemprop="url">2. Dataset, Sampler, Dataloader</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:46:58" itemprop="dateCreated datePublished" datetime="2021-09-22T23:46:58-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>913</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>torch.utils.data.Dataset</code>对象：存储样本信息。<br />
<code>torch.utils.data.Sampler</code>对象：提供一种遍历数据集所有元素索引的方式。<br />
<code>torch.utils.data.DataLoader</code>对象：包装一个Dataset的迭代器，用于更加方便地得到样本。</p>
<ul>
<li>总地来说：<br />
Dataloader 负责总的调度，<br />
命令 Sampler 定义遍历索引的方式，<br />
然后用索引去 Dataset 中提取元素。<br />
于是就实现了对给定数据集的遍历。</li>
</ul>
<h2 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h2>
<p><code>torch.utils.data.Dataset</code>对象有两种：「Map-style dataset」和 「Iterable-style dataset」。<br />
每个样本有对应的索引，通过输入具体的索引，就能够得到自定义的样本信息。</p>
<h3 id="map-style-dataset"><a class="markdownIt-Anchor" href="#map-style-dataset"></a> Map-style dataset</h3>
<p>自定义的Dataset对象必须实现<code>__init__()</code>, <code>__len__()</code>和<code>__getitem__(self, idx)</code>方法，作为一种存储和读取数据的协议。</p>
<p>「Map-style dataset」并不是直接读取的数据集，而是读取每个样本的索引。<br />
即后续取样本时，通过借助索引，即<code>__getitem__()</code>协议的方式读取样本。大致流程为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for indices in batch_sampler:</span><br><span class="line">    yield collate_fn([dataset[i] for i in indices])</span><br></pre></td></tr></table></figure>
<h4 id="参数"><a class="markdownIt-Anchor" href="#参数"></a> 参数：</h4>
<ul>
<li><code>__init__()</code>：在创建dataset时被调用一次</li>
<li><code>__len__()</code>：返回样本数量</li>
<li><code>__getitem__(self, idx)</code>：加载并且返回一个下标为<code>idx</code>的样本。<br />
对应的操作符就是索引符号<code>[]</code>，即<code>dataset.__getitem__(idx)</code>和<code>dataset[idx]</code>操作一致。</li>
</ul>
<h3 id="iterable-style-dataset"><a class="markdownIt-Anchor" href="#iterable-style-dataset"></a> Iterable-style dataset</h3>
<p>并不需要实现<code>__getitem__()</code>方法和<code>__len__()</code>方法。Dataset本身更像是一个迭代器，使用<code>__iter()__</code>协议。<br />
不同于「map-style dataset」，该类型数据集下样本的索引相互独立，注意使用 <code>DataLoader</code> 载入数据时，如果 <code>num_worker</code> 数量大于1，则需要独立分配索引。</p>
<h2 id="dataloader"><a class="markdownIt-Anchor" href="#dataloader"></a> DataLoader</h2>
<ul>
<li><code>Dataset</code>一次只能检索到一个样本，但是我们普遍以batch为单位进行训练。所以我们使用<code>DataLoader</code>作为一个迭代器，返回batch单位的数据。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)</span><br></pre></td></tr></table></figure>
<ul>
<li>DataLoader的每一次迭代过程都会返回一个batch大小的样本特征数组和样本标签数组。<br />
<code>train_features, train_labels = next(iter(train_dataloader))</code></li>
</ul>
<p>定义后查看一个 batch内部数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># have a look at dataloader</span><br><span class="line"></span><br><span class="line">train_data_iter = iter(train_dataloader)</span><br><span class="line">batch_data = next(train_data_iter)</span><br><span class="line">text_id_list, text_list, batch_input_ids, \</span><br><span class="line">batch_attention_mask, batch_token_type_ids, \</span><br><span class="line">tok2char_span_list, batch_shaking_tag = batch_data</span><br></pre></td></tr></table></figure>
<h3 id="参数-2"><a class="markdownIt-Anchor" href="#参数-2"></a> 参数</h3>
<ul>
<li><code>shuffle=True</code>时，调用<code>RandomSampler</code>进行随机索引。</li>
<li><code>sampler</code>定义从数据集中提取样本的策略，应该传入一个<code>Sampler</code>对象。如果指定<code>sampler</code>对象，则<code>shuffle</code>参数必须为<code>False</code>。</li>
<li><code>batch_sampler</code>，和<code>sampler</code>类似，但是一般传入<code>BatchSampler</code>对象，每次返回一个<code>batch</code>大小的索引。和<code>batch_size</code>，<code>shuffle</code>参数互斥。</li>
<li><code>drop_last=True</code>时，会自动删除最后一个不完整的batch。</li>
<li><code>collate_fn=&lt;function default_collate&gt;</code>，定义如何取样本，我们可以定义自己的函数来准确地实现想要的功能。<br />
从「Map-style dataset」处取样本的伪代码可以看到，<code>collate_fn</code>这个函数有一个默认的输入<code>[dataset[i] for i in indices]</code>，list的长度是一个batch size，list中的每个元素都是<code>__getitem__()</code>得到的结果。所以在重写该方法时，注意有一个默认的参数<code>def collate(self, examples)</code>。</li>
<li>·<code>num_workers</code>：使用多少个进行处理数据，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/523239005">使用unix系统时请注意BUG</a>。</li>
</ul>
<h2 id="demo"><a class="markdownIt-Anchor" href="#demo"></a> Demo</h2>
<ol>
<li>使用一个自定义的<code>read_example_form_file(args, mode)</code> 方法，读取所有的样本，生成一个样本列表。<br />
样本列表的结构为：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">examples = [</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;sentence&quot;: sentence,</span><br><span class="line">        &quot;label&quot;: label,</span><br><span class="line">        &quot;feature&quot;: feature</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>
<p>自定义<code>my_Dataset</code>继承<code>torch.utils.data.Dataset</code>，</p>
<ol>
<li>实现<code>__init__()</code>，<code>__len_()</code>，<code>__getitem__()</code>方法。</li>
<li>自定义一个<code>collate(self, example_list)</code>方法，后续直接传入<code>DataLoader</code>的<code>collate_fn</code>参数。<br />
在<code>collate()</code>内部实现 tokenizer 和 padding 操作，返回<code>raw_text_list, batch_input_ids, batch_attention_mask, batch_segment_ids, batch_labels, batch_hand_features</code>，这样训练时，直接取出即可交给模型，计算损失，进行训练。</li>
</ol>
</li>
<li>
<p>直接使用<code>torch.utils.data.DataLoader</code>封装<code>my_Dataset</code>和<code>my_Dataset.collate</code>即可。<br />
注意传入的是<code>collate</code>(<code>&lt;function&gt;</code>)，不是<code>collate()</code>。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.4.%20Torch.tensor%E6%8B%BC%E6%8E%A5%E5%92%8C%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.4.%20Torch.tensor%E6%8B%BC%E6%8E%A5%E5%92%8C%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">1.4. Torch.tensor 拼接和分割</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:38" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:38-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>365</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="堆砌拼接"><a class="markdownIt-Anchor" href="#堆砌拼接"></a> 堆砌拼接</h2>
<h3 id="stack"><a class="markdownIt-Anchor" href="#stack"></a> stack()</h3>
<p>在一个新的维度上拼接，注意会<strong>生成新的维度</strong>。</p>
<p>参数</p>
<ul>
<li><code>tensors (sequence of Tensors)</code>：tensor 列表，所有 tensor 的维度大小必须一致。</li>
<li><code>dim=0 (int)</code>：拼接的维度</li>
</ul>
<h3 id="cat"><a class="markdownIt-Anchor" href="#cat"></a> cat()</h3>
<p>在指定维度上拼接，<strong>不会生成新的维度</strong>。</p>
<p>参数</p>
<ul>
<li><code>tensors (sequence of Tensors)</code>：tensor 列表，所有 tensor ，除了待拼接的维度外，其他维度的大小必须一致。</li>
<li><code>dim=0 (int, optional)</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.stack([a,b])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.stack([a,b])</span><br><span class="line">tensor([[[1., 1., 1.],</span><br><span class="line">         [1., 1., 1.]],</span><br><span class="line">        [[0., 0., 0.],</span><br><span class="line">         [0., 0., 0.]]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.cat([a,b])</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>
<h2 id="分割"><a class="markdownIt-Anchor" href="#分割"></a> 分割</h2>
<h3 id="split"><a class="markdownIt-Anchor" href="#split"></a> split()</h3>
<p>分割tensor，返回分割后的子tensor列表。</p>
<p>参数：</p>
<ul>
<li><code>tensor (Tensor)</code></li>
<li><code>split_size_or_sections (int) or (list(int))</code>
<ul>
<li>如果是整数<code>split_size</code>，则分成多个维度为<code>split_size</code>的tensor。（如果不能均分，则最后一个子tensor的维度会小于<code>split_size</code>）。</li>
<li>如果是列表<code>split(list(int))</code>，则按照列表的元素大小来分割。</li>
</ul>
</li>
<li><code>dim=0 (int)</code>：分割维度</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.arange(10).reshape(5,2)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [2, 3],</span><br><span class="line">        [4, 5],</span><br><span class="line">        [6, 7],</span><br><span class="line">        [8, 9]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.split(a, 2)</span><br><span class="line">(tensor([[0, 1],</span><br><span class="line">         [2, 3]]),</span><br><span class="line"> tensor([[4, 5],</span><br><span class="line">         [6, 7]]),</span><br><span class="line"> tensor([[8, 9]]))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.split(a, [1,4])</span><br><span class="line">(tensor([[0, 1]]),</span><br><span class="line"> tensor([[2, 3],</span><br><span class="line">         [4, 5],</span><br><span class="line">         [6, 7],</span><br><span class="line">         [8, 9]]))</span><br></pre></td></tr></table></figure>
<h3 id="chunk"><a class="markdownIt-Anchor" href="#chunk"></a> chunk()</h3>
<p>同样是分割函数，和<code>split()</code>方法类似，区别在于传入参数<code>chunks</code>，不同于参数<code>split_size_or_sections</code>。</p>
<p>参数</p>
<ul>
<li><code>tensor (Tensor)</code></li>
<li><code>chunks (int)</code>：返回的分割tensor的数量，如果无法整除，则最后一个子tensor的维度为余数。</li>
<li><code>dim=0 (int)</code>：分割维度</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.5.%20Torch.tensor%E6%89%A9%E5%A2%9E%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%B9%BF%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.5.%20Torch.tensor%E6%89%A9%E5%A2%9E%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%B9%BF%E6%92%AD/" class="post-title-link" itemprop="url">1.5. Torch.tensor扩增，压缩和广播</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:38" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:38-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>851</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>对任意一个tensor来说，其大小等于所有维度大小的乘积。<br />
所以任意增加或者减少一个大小为1的维度，不会改变原始大小。</p>
<h2 id="unsqueeze"><a class="markdownIt-Anchor" href="#unsqueeze"></a> unsqueeze()</h2>
<p>增加张量的维度<br />
不会新建tensor，只是返回一个新的视图。<br />
结果tensor和原tensor的内存是共享的，如果后续需要修改可以使用<code>contiguous()</code>方法。</p>
<p>参数</p>
<ul>
<li><code>input (Tensor)</code>：</li>
<li><code>dim (int)</code>：<code>[-input.dim()-1, input.dim()+1)</code>支持负数</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.unsqueeze(x, 0)</span><br><span class="line">tensor([[ 1,  2,  3,  4]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.unsqueeze(x, 1)</span><br><span class="line">tensor([[ 1],</span><br><span class="line">        [ 2],</span><br><span class="line">        [ 3],</span><br><span class="line">        [ 4]])</span><br></pre></td></tr></table></figure>
<h2 id="squeeze"><a class="markdownIt-Anchor" href="#squeeze"></a> squeeze()</h2>
<p>压缩：删除大小为1的维度<br />
不会新建tensor，只是返回一个新的视图。<br />
结果tensor和原tensor的内存是共享的，如果后续需要修改可以使用<code>contiguous()</code>方法。</p>
<p>参数</p>
<ul>
<li><code>input (Tensor)</code></li>
<li><code>dim=None (int, optional)</code>
<ul>
<li>如果为<code>None</code>，则删除所有大小为1的维度</li>
<li>如果不为<code>None</code>，则删除选定的维度。如果选定维度大小不为1，则unchanged。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([2, 1, 2, 1, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.unsqueeze(-1).data_ptr() == x.data_ptr() # 内存共享</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; y = torch.squeeze(x)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([2, 2, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; y = torch.squeeze(x, 0)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([2, 1, 2, 1, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; y = torch.squeeze(x, 1)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([2, 2, 1, 2])</span><br></pre></td></tr></table></figure>
<h2 id="tensorexpand"><a class="markdownIt-Anchor" href="#tensorexpand"></a> tensor.expand()</h2>
<p>不会新建tensor，只是返回一个新的视图。<br />
结果tensor和原tensor的内存是共享的，如果后续需要修改可以使用<code>contiguous()</code>方法。</p>
<p>参数</p>
<ul>
<li><code>*sizes (torch.Size or int...)</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([3, 1])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.expand(3, 4)</span><br><span class="line">tensor([[ 1,  1,  1,  1],</span><br><span class="line">        [ 2,  2,  2,  2],</span><br><span class="line">        [ 3,  3,  3,  3]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension</span><br><span class="line">tensor([[ 1,  1,  1,  1],</span><br><span class="line">        [ 2,  2,  2,  2],</span><br><span class="line">        [ 3,  3,  3,  3]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x.data_ptr() == x.expand(3,4).data_ptr() # 内存共享</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<h2 id="torchrepeat_interleave"><a class="markdownIt-Anchor" href="#torchrepeat_interleave"></a> torch.repeat_interleave()</h2>
<p>将输入张量，按照指定维度重复指定次数，<strong>一个元素一个元素地重复</strong>（<code>tensor.repeat()</code>是一组一组重复）。<br />
返回的数组与输入数组维数相同，并且除了给定的维度dim，其他维度大小与输入数组相应维度大小相同。</p>
<p>参数</p>
<ul>
<li>input (Tensor)</li>
<li>repeats (Tensor or int)：元素的重复次数</li>
<li>dim=None (int, optional)：重复元素的维度；默认情况下将把给定的输入张量展平（flatten）为向量，然后将每个元素重复repeats次，并返回重复后的张量。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.arange(10).reshape(2, 5)</span><br><span class="line">tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; # repeat_interleave 是元素级别重复</span><br><span class="line">&gt;&gt;&gt; torch.repeat_interleave(a, 2, dim=0)</span><br><span class="line">tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9],</span><br><span class="line">        [5, 6, 7, 8, 9]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; # repeat 是整个 tensor 级别重复</span><br><span class="line">&gt;&gt;&gt; a.repeat(2, 2)</span><br><span class="line">tensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9],</span><br><span class="line">        [0, 1, 2, 3, 4, 0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])</span><br></pre></td></tr></table></figure>
<h2 id="广播"><a class="markdownIt-Anchor" href="#广播"></a> 广播</h2>
<p>在运行tensor的四则运算时，需要两个tensor维度相同。<br />
维度不同时，会有广播属性，即大小为1的维度会被复制（类似expand）为和另一个tensor相同的大小。</p>
<p>广播的条件为：</p>
<ul>
<li>数组拥有相同形状：无需广播。</li>
<li>后缘维度相同。</li>
<li>后缘维度不同，但是其中为1。</li>
</ul>
<p>若条件不满足，会抛出 “RuntimeError” 异常。</p>
<p>这时就需要使用<code>unsqueeze()</code>方法对张量之间进行扩维，使得后缘维度至少有一个为1。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.randn(3,2,4)</span><br><span class="line">&gt;&gt;&gt; b = torch.randn(3,4)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a+b</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a+b.unsqueeze(1)</span><br><span class="line">tensor([[[-1.6027, -0.9681, -0.2714,  0.4159],</span><br><span class="line">         [-1.5749,  4.0289,  0.6376, -1.0906]],</span><br><span class="line">        [[ 2.4601, -2.3880,  1.4608,  1.2630],</span><br><span class="line">         [ 0.5983, -2.4189,  0.7459,  1.2242]],</span><br><span class="line">        [[-0.6069, -1.6323,  0.9585,  0.9418],</span><br><span class="line">         [ 1.4031, -1.7707,  1.7068,  0.7386]]])</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.1.%20Torch.tensor%E5%88%87%E7%89%87%E5%92%8C%E7%B4%A2%E5%BC%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.1.%20Torch.tensor%E5%88%87%E7%89%87%E5%92%8C%E7%B4%A2%E5%BC%95/" class="post-title-link" itemprop="url">1.1 torch.tensor 切片和索引</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:32" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:32-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>209</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="索引"><a class="markdownIt-Anchor" href="#索引"></a> 索引</h2>
<ul>
<li>使用<code>[]</code>，不同维度之间使用<code>,</code>隔开。</li>
<li>和 numpy 类似</li>
<li>索引和切片后的tensor，和原始的tensor共享一个内存区域。可以使用<code>clone()</code>方法得到一个新的副本，进行后续修改操作。</li>
<li>支持 <strong>掩码</strong> 操作：传入一个相同大小的 boolean tensor，对应选择位置为 True；或者 8 位无符号整数 1。<br />
注意，返回的是一个一维 tensor！但是同样可以通过这个tensor进行后续操作。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor(range(6)).reshape(2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[0, 1, 2],</span><br><span class="line">        [3, 4, 5]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a&gt;3</span><br><span class="line">tensor([[False, False, False],</span><br><span class="line">        [False,  True,  True]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a[a&gt;3]</span><br><span class="line">tensor([4, 5])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a[a&gt;3]=100</span><br><span class="line">a</span><br><span class="line">tensor([[  0,   1,   2],</span><br><span class="line">        [  3, 100, 100]])</span><br></pre></td></tr></table></figure>
<h2 id="拼接"><a class="markdownIt-Anchor" href="#拼接"></a> 拼接</h2>
<p>与<code>numpy</code>中的<code>axis</code>不同 ，<code>tensor.tensor</code>轴的参数使用<code>dim</code>。下面例子中，torch中的<code>cat</code>方法即可以看作<code>np.concatnate</code>方法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; t1 = torch.cat([tensor, tensor, tensor], dim=1)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.2.%20Torch.tensor%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.2.%20Torch.tensor%E8%AE%A1%E7%AE%97/" class="post-title-link" itemprop="url">1.2 torch.tensor 计算</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:30" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:30-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>995</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="tensor内部"><a class="markdownIt-Anchor" href="#tensor内部"></a> tensor内部</h2>
<h3 id="平方根"><a class="markdownIt-Anchor" href="#平方根"></a> 平方根</h3>
<ul>
<li>内部方法：<code>tensor.sqrt()</code></li>
<li>torch函数形式：<code>torch.sqrt()</code><br />
这两个方法需要设置返回tensor来接收。</li>
<li>原地操作：<code>tensor.sqrt_</code></li>
</ul>
<h3 id="求和"><a class="markdownIt-Anchor" href="#求和"></a> 求和</h3>
<p><code>tensor.sum()</code>和<code>torch.sum()</code><br />
注意可以传入维度列表参数选定维度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[  0,   1],</span><br><span class="line">        [  2,   3],</span><br><span class="line">        [100, 100]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.sum(1)</span><br><span class="line">tensor([  1,   5, 200])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.sum(0)</span><br><span class="line">tensor([102, 104])</span><br></pre></td></tr></table></figure>
<h2 id="tensor间"><a class="markdownIt-Anchor" href="#tensor间"></a> tensor间</h2>
<p>注意两 tensor 的 dtype 一致。</p>
<h3 id="element-wise"><a class="markdownIt-Anchor" href="#element-wise"></a> element wise</h3>
<p><code>= - * /</code> 或者 <code>add()</code>,<code>sub()</code>,<code>mul()</code>,<code>dic()</code></p>
<p>如果张量中只有一个元素，可以使用<code>tensor.item()</code>得到这个单一元素。</p>
<h3 id="矩阵乘法点乘方法"><a class="markdownIt-Anchor" href="#矩阵乘法点乘方法"></a> 矩阵乘法/点乘方法</h3>
<p><code>tensor1 @ tensor2.T</code> ，是 <code>matmul</code> 的缩写。</p>
<p><code>tensor1.matmul(tensor2.T)</code> / <code>torch.matmul()</code></p>
<ul>
<li>如果两个tensor都是一维的，则为点乘运算，每个元素对应相乘求和。</li>
<li>如果两个都是二维的，那么就如同<code>torch.mm</code>。</li>
<li>如果第一个入参是一维的，第二个入参是二维的，则第一个参数增加一个一维，做<strong>矩阵乘法</strong>，结果然后去掉一维。</li>
<li>如果第一个入参是二维的，第二个是一维，则将第二个参数扩展一维，做矩阵乘法。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor([[1,2],[2,3]])</span><br><span class="line">&gt;&gt;&gt; b = torch.tensor([1,-1])</span><br><span class="line">&gt;&gt;&gt; a.matmul(b)</span><br><span class="line">tensor([-1, -1])</span><br></pre></td></tr></table></figure>
<ul>
<li>最后如果维度不同，涉及到batch乘法，<strong>（广播后）batch维度匹配</strong>：
<ul>
<li>两边shape的rank要相同，最后两维是用来做mm计算的</li>
<li>如果shape的rank不同，则可以通过广播使得batch维度相同。<br />
如<code>matmul(shape(10,4,2),shape(2,3))</code>则变为 <code>matmul(shape(10,4,2),shape(1,2,3))</code></li>
</ul>
</li>
</ul>
<p><code>tensor1.mm(tensor2.T)</code> / <code>torch.mm()</code></p>
<ul>
<li>不建议使用：仅提供矩阵相乘使用，使用范围较为狭窄。</li>
<li>如果维度超过二维，则会报错。RuntimeError: self must be a matrix</li>
</ul>
<p><code>tensor1.bmm(tensor2)</code> / <code>torch.bmm()</code><br />
batch-mm，不推荐，因为不支持自动broadcast。</p>
<h3 id="torcheinsum"><a class="markdownIt-Anchor" href="#torcheinsum"></a> torch.einsum()</h3>
<p>利用了爱因斯坦求和简介高效的表示方法，从而表示任何复杂的矩阵计算操作。</p>
<h4 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h4>
<p>缩并（Contraction）：对于更大维度的tensor乘法，需要决定tensor乘积结果需要沿着哪些维度求和。</p>
<p>现在考虑情况 <code>shape(3,3,3,2)</code> 和 <code>shape(2,4,2,3)</code> ，将最后两维度看作instance。<br />
我们想求，前后每两个instance的矩阵乘法结果。即答案为<code>shape(3,3,2,4,3,3)</code>，用<code>@</code>是无法求的，因为这不是batch乘法。<br />
同时即使维度相同，有时候也表示不同的batch/特征维度，还是两两相乘。</p>
<p>对于<code>einsum(A,B) =C</code>，首先将对应维度的下标（和维度无关）分为三类：</p>
<ul>
<li>ABC都共享：则下标对应的一系列元素需要作两两乘积（张量积）。</li>
<li>AB共享，C不共享：下表对应的元素需要做乘积求和（内积）。</li>
<li>AB共享，且维度相同，C只出现一次：按位置做乘法。</li>
</ul>
<h4 id="方法"><a class="markdownIt-Anchor" href="#方法"></a> 方法</h4>
<ul>
<li>输入的参数：
<ul>
<li><code>quation (str)</code>：输入tensor的下标和输出tensor的形状，以不同字母作区分。</li>
<li><code>operands (Tensor, [Tensor, …])</code>：数量及维度需与前面对应</li>
</ul>
</li>
<li>规则：
<ul>
<li>两个矩阵重复的index，会作乘积求和</li>
<li>省略的index会被加和</li>
<li>可以任意转换维度</li>
</ul>
</li>
<li>自由标和哑标：输入标记中仅出现一次的下标为自由标（free index），重复出现的下标为哑标（dummy/summation index），哑标对应的维度分量将被规约消去。</li>
<li>广播维度：省略号<code>...</code>表示维度的广播分量，例如，<code>i…j</code> 表示首末分量除外的维度需进行广播对齐</li>
<li><a target="_blank" rel="noopener" href="https://viatorsun.blog.csdn.net/article/details/122710515?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ELandingCtr%7ERate-1.queryctrv4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ELandingCtr%7ERate-1.queryctrv4&amp;utm_relevant_index=2">具体计算参考博客</a></li>
</ul>
<h4 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h4>
<p>一些乘法操作：</p>
<ul>
<li>Matrix Mult</li>
<li>Element-wise Mult/哈达玛积</li>
<li>Permutation</li>
<li>Dot product/Inner product：对应矩阵元素的积之和（形状相同）,sum(element_wise_dot)</li>
<li>Outer product/张量积: shape(m),shape(n)-&gt;shape(m,n)</li>
<li>Specific Summation</li>
<li>Batch Matrix Mult</li>
<li>教程：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XZ4y1Q7Ya?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1XZ4y1Q7Ya?spm_id_from=333.337.search-card.all.click</a></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 3,  4,  5]],</span><br><span class="line">        [[ 6,  7,  8],</span><br><span class="line">         [ 9, 10, 11]]])</span><br></pre></td></tr></table></figure>
<ul>
<li>permutation</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk-&gt;jik&quot;, a)</span><br><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 6,  7,  8]],</span><br><span class="line">        [[ 3,  4,  5],</span><br><span class="line">         [ 9, 10, 11]]])</span><br></pre></td></tr></table></figure>
<ul>
<li>column sum</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk-&gt;ik&quot;, a)</span><br><span class="line">tensor([[ 3,  5,  7],</span><br><span class="line">        [15, 17, 19]])</span><br></pre></td></tr></table></figure>
<ul>
<li>row sum</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk-&gt;ij&quot;, a)</span><br><span class="line">tensor([[ 3, 12],</span><br><span class="line">        [21, 30]])</span><br></pre></td></tr></table></figure>
<ul>
<li>Matrix Mult, <code>matmul()</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([2, 3, 4])</span><br><span class="line">&gt;&gt;&gt; b.shape</span><br><span class="line">torch.Size([2, 4, 3])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.einsum(&quot;bnk,bkl-&gt;bnl&quot;, a,b).shape</span><br><span class="line">torch.Size([2, 3, 3])</span><br></pre></td></tr></table></figure>
<ul>
<li>Dot product</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ijk,ijk-&gt;&quot;,a,a)</span><br><span class="line">tensor(506)</span><br></pre></td></tr></table></figure>
<ul>
<li>Element-wise Mult</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.einsum(&quot;ij,ij-&gt;ij&quot;,a,a)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/22/Coding/Pytorch/1.3.%20Torch.tensor%E6%9E%81%E5%80%BC%E5%92%8C%E6%8E%92%E5%BA%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/22/Coding/Pytorch/1.3.%20Torch.tensor%E6%9E%81%E5%80%BC%E5%92%8C%E6%8E%92%E5%BA%8F/" class="post-title-link" itemprop="url">1.3 torch.tensor 极值和排序</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-22 23:08:30" itemprop="dateCreated datePublished" datetime="2021-09-22T23:08:30-07:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coding/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>194</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="argmax-argmin"><a class="markdownIt-Anchor" href="#argmax-argmin"></a> argmax() / argmin()</h2>
<p><code>tensor.argmax()</code>，传入维度<br />
<code>torch.argmax()</code>，传入tensor和维度<br />
返回的是每个维度对应的序号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor(</span><br><span class="line">            [[0, 1, 2],</span><br><span class="line">            [3, 10, 5],</span><br><span class="line">            [60, 7, 1]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.argmax(0)</span><br><span class="line">tensor([2, 1, 1])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.argmax(1)</span><br><span class="line">tensor([2, 1, 0])</span><br></pre></td></tr></table></figure>
<h2 id="max-min"><a class="markdownIt-Anchor" href="#max-min"></a> max() / min()</h2>
<p>返回位置和具体的值，返回的是元组形式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a.max(1)</span><br><span class="line">torch.return_types.max(</span><br><span class="line">    values=tensor([ 2, 10, 60]),</span><br><span class="line">    indices=tensor([2, 1, 0]))</span><br></pre></td></tr></table></figure>
<p>使用<code>a.max(1)[1]</code>来得到最大值结果。</p>
<h2 id="sort"><a class="markdownIt-Anchor" href="#sort"></a> sort()</h2>
<p><code>tensor.sort()</code> / <code>torch.sort()</code><br />
排序并且返回一个<code>namedtuple</code>:<code>(values, indices)</code>，即排序后的tensor，和tensor元素的原始位置。</p>
<p>参数：</p>
<ul>
<li><code>dim (int, optional)</code></li>
<li><code>descending = False (bool, optional)</code>：是否降序，默认为False（升序）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ 0,  1,  2],</span><br><span class="line">        [ 3, 10,  5],</span><br><span class="line">        [60,  7,  1]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a.sort(0)</span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values=tensor([[ 0,  1,  1],</span><br><span class="line">                [ 3,  7,  2],</span><br><span class="line">                [60, 10,  5]]),</span><br><span class="line">indices=tensor([[0, 0, 2],</span><br><span class="line">                [1, 2, 0],</span><br><span class="line">                [2, 1, 1]]))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a = torch.ones(2,3)</span><br></pre></td></tr></table></figure>
<p>使用<code>a.sort(1)[0]</code>得到排序后结果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/24/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/26/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">A foolish, slow learner.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">356</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YiandLi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YiandLi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yiliiiii" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yiliiiii" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">615k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">51:17</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
