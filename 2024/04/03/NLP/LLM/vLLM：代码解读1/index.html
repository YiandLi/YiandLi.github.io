<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":20,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="两种调用方式与内核引擎LLMEngine：">
<meta property="og:type" content="article">
<meta property="og:title" content="vLLM：代码解读1">
<meta property="og:url" content="http://example.com/2024/04/03/NLP/LLM/vLLM%EF%BC%9A%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/index.html">
<meta property="og:site_name" content="Yili">
<meta property="og:description" content="两种调用方式与内核引擎LLMEngine：">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/NLP/LLM/vLLM/vllm_coding1.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-eee036cd8edbc2b94d8758721b9809e8_1440w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-f1c6d2e41bd27b8424e1824539748354_720w.webp">
<meta property="og:image" content="http://example.com/images/NLP/LLM/vLLM/vllm_coding2.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-83b8cc298c0f2369e75ba3f2f7ef06a2_1440w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-9df49b61fa249c0a6ff05a8d64858366_1440w.webp">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-e8b544bc41596a0de9f814d47ca347cf_1440w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-ab509dc88461321fe2b323fe04b43285_1440w.webp">
<meta property="og:image" content="http://example.com/2024/04/03/NLP/LLM/vLLM%EF%BC%9A%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/image.png">
<meta property="article:published_time" content="2024-04-03T05:12:07.000Z">
<meta property="article:modified_time" content="2024-04-21T06:53:50.829Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/NLP/LLM/vLLM/vllm_coding1.png">

<link rel="canonical" href="http://example.com/2024/04/03/NLP/LLM/vLLM%EF%BC%9A%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>vLLM：代码解读1 | Yili</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yili</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Suggest Google Chrome for better math Reading.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/03/NLP/LLM/vLLM%EF%BC%9A%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="A foolish, slow learner.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yili">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          vLLM：代码解读1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-04-03 13:12:07" itemprop="dateCreated datePublished" datetime="2024-04-03T13:12:07+08:00">2024-04-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/LLM-Inference/" itemprop="url" rel="index"><span itemprop="name">LLM Inference</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>14 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>两种调用方式与内核引擎LLMEngine：</p>
<p><img src="/images/NLP/LLM/vLLM/vllm_coding1.png" alt="alt text" /></p>
<span id="more"></span>
<h1 id="代码逻辑"><a class="markdownIt-Anchor" href="#代码逻辑"></a> 代码逻辑</h1>
<p>做demo用的api server官方已经不再维护了，openai_api_server才是官方推荐的使用方式，user custom server目前还没有实现。</p>
<p>向用户提供了两种调用方法：</p>
<ol>
<li>
<p>Offline Batched Inference -&gt; LLMEngine<br />
同步，离线批处理，推理引擎在固定 batch、固定输⼊输出 token 数量的前提下的推理。<br />
表面上是在做“同步”推理，也即 <code>batch_size</code> 是静态固定的。但推理内核引擎（LLMEngine）在实际运作时，<code>batch_size</code> 是可以动态变更的。<br />
将LLMEngine包装成离线批处理形式后，所有的数据必须等到一起做完推理才能返给我们。所以从体感上，我们可能很难感知到 Dynamic Batching 。</p>
</li>
<li>
<p>API Server For Online Serving（异步，在线推理服务） -&gt; AsyncLLMEngine -&gt; LLMEngine<br />
⽤⼾输⼊的 prompt ⻓度以及模型回复的 token 数量是动态变化的，是真实应⽤中的推理。</p>
</li>
</ol>
<p>vLLM 的 Dynamic Batching ；维护三个队列：</p>
<ol>
<li>waiting：由所有等待 Prefill 的请求组成</li>
<li>running：由所有即将或者进⾏推理的请求组成</li>
<li>swapped：由所有暂时换出到 CPU 内存中的请求组成</li>
</ol>
<p>所有样本都肯定经过 waiting 和 running 队列。</p>
<ol>
<li>batch中的每一条数据，会被先放到一个「waiting 队列」中。</li>
<li>vLLM 会用 <strong>自己的调度策略</strong> 从「waiting 队列」中依次取样本，放入「running队列」中。</li>
<li>在每1个推理阶段，vLLM对「running 队列」中的数据做推理。</li>
<li>如果「running 队列」中样本生成结束，则 pop 出队列，返回给用户或者本地保存，并释放它占据的物理块显存。<br />
这时，waiting队列中的数据就可以继续append进running队列中，做下1个阶段的推理。</li>
</ol>
<p>异步在线服务也类似，主要特点为： <strong>先来先服务（First-Come-First-Serve, FCFS），如有抢占的需要，后来的请求先被抢占（preemption）</strong></p>
<ol>
<li>请求首先进入LLMEngine调度器（Scheduler）的「waiting 队列」中。</li>
<li>在每一次进行 step 时，调度器再根据设定的策略，决定哪些数据可以进入「running 队列」进行推理。</li>
<li>先推理完成的数据就可以先发给客户端了（如果采用流式传输，也可以生成多少先发多少）。</li>
</ol>
<h2 id="vllm代码整体架构"><a class="markdownIt-Anchor" href="#vllm代码整体架构"></a> vLLM代码整体架构</h2>
<p><img src="https://pic1.zhimg.com/80/v2-eee036cd8edbc2b94d8758721b9809e8_1440w.webp" alt="" /></p>
<ul>
<li>
<p>Centralized Controller : 调度器(Scheduler) ，和LLMEngine所在的进程是同一个，且两者都是在CPU上的。</p>
<ul>
<li>调度器的主要作用：
<ul>
<li>每个时间步骤，决定哪些数据进行推理，哪些新数据进行 prefill，哪些数据从显存 offload 到 cpu …</li>
<li>分配 KV Cache物理块 ，只是分配了物理块的id，而不是物理块本身。</li>
<li>维护 BlockSpaceManager ，负责管理BlockAllocator（实际参与分配物理块的类）。BlockAllocator又分成gpu和cpu两种类型，分别管理这两类设备上的物理块。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Distributed Workers : 每个 worker 管理一块 gpu</p>
<ul>
<li>绿色部分：指定了用什么方法管控这些Workers，负责分布式环境的初始化
<ul>
<li><code>cpu_executor</code>：（较少用），使用cpu做推理时可考虑</li>
<li><code>gpu_executor</code>：单卡（ <code>world_size = 1</code> ）的情况下可用</li>
<li><code>ray_gpu_executor</code>：使用ray这个分布式计算框架实现的executor，适用于多卡环境</li>
</ul>
</li>
<li>Worker：在硬件上，它指gpu；在代码上，它指的是Worker实例（每个gpu上的进程维护自己的Worker实例）
<ul>
<li><code>CacheEngine</code> ：负责管控 gpu/cpu 上的 KV cache 物理块</li>
<li><code>Worker.model</code> ：负责加载模型，并执行推理。PagedAttention的相关逻辑，就维护这个实例关联的代码下。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="llm-离线"><a class="markdownIt-Anchor" href="#llm-离线"></a> LLM 离线</h1>
<p>初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:  <span class="string">&quot;&quot;&quot;离线：根据 prompt 和采样参数生成文本&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">  self, model: <span class="built_in">str</span>,  ... </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        engine_args = EngineArgs( model=model,  ... )</span><br><span class="line">        <span class="comment"># 初始化 LLMEngine</span></span><br><span class="line">        self.llm_engine = LLMEngine.from_engine_args(engine_args)</span><br><span class="line">        self.request_counter = Counter()</span><br></pre></td></tr></table></figure>
<p>generate 方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        prompts: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        sampling_params: <span class="type">Optional</span>[SamplingParams] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        prompt_token_ids: <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        use_tqdm: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[RequestOutput]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Generates the completions for the input prompts.&quot;&quot;&quot;</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_requests):</span><br><span class="line">            prompt = prompts[i] <span class="keyword">if</span> prompts <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            token_ids = <span class="literal">None</span> <span class="keyword">if</span> prompt_token_ids <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> prompt_token_ids[</span><br><span class="line">                i]</span><br><span class="line">            self._add_request(prompt, sampling_params, token_ids)</span><br><span class="line">        <span class="keyword">return</span> self._run_engine(use_tqdm)</span><br></pre></td></tr></table></figure>
<p>可以看到，我们一次性将 prompt 全部送进去，内部则是不断调用 <code>self._add_request()</code> 方法，将请求添加到 <code>LLMEngine</code> 实例。<br />
然后调用 <code>self._run_engine()</code> 方法完成请求的处理。</p>
<p>真实离线生成方法 <code>_run_engine()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run_engine</span>(<span class="params">self, use_tqdm: <span class="built_in">bool</span></span>) -&gt; <span class="type">List</span>[RequestOutput]:</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># Run the engine.</span></span><br><span class="line">        outputs: <span class="type">List</span>[RequestOutput] = []</span><br><span class="line">        <span class="keyword">while</span> self.llm_engine.has_unfinished_requests():</span><br><span class="line">            step_outputs = self.llm_engine.step()</span><br><span class="line">            <span class="keyword">for</span> output <span class="keyword">in</span> step_outputs:</span><br><span class="line">                <span class="keyword">if</span> output.finished: outputs.append(output)</span><br><span class="line">        ...</span><br><span class="line">        outputs = <span class="built_in">sorted</span>(outputs, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x.request_id))</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>不断调用 <code>self.llm_engine.step()</code> 方法，不断 step-by-step &amp; iterative-level 进行 token 的生成。</p>
<p>而判断是否停止的方法 <code>self.llm_engine.has_unfinished_requests()</code> ，则需要 <code>self.scheduler</code> 的 <code>has_unfinished_seqs()</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">has_unfinished_seqs</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.waiting) != <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(self.running) != <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(self.swapped) != <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>就是三个列表是否均为空。</p>
<h1 id="llmengine"><a class="markdownIt-Anchor" href="#llmengine"></a> LLMEngine</h1>
<p>两个核心组件</p>
<p><img src="https://pic1.zhimg.com/80/v2-f1c6d2e41bd27b8424e1824539748354_720w.webp" alt="LLMEngine 核心组件" /></p>
<p>总体初始化流程为：<br />
<img src="/images/NLP/LLM/vLLM/vllm_coding2.png" alt="" /></p>
<h2 id="scheduler"><a class="markdownIt-Anchor" href="#scheduler"></a> Scheduler</h2>
<p>Scheduler ：逻辑组件 - 负责请求调度，用于决定下一个 iteration 处理哪些请求</p>
<p>使用 iterative-level 策略对请求进行调度（选择要被处理的请求），被调度的请求在生成一个 token 后会被重新调度。<br />
得益于 iterative-level 策略，vLLM 能够在每一轮新的迭代时选择不固定数量的请求进行处理（即 batch size 每次都不一定相同），因此它能够尽可能多地处理请求。</p>
<!-- <details>
<summary> <code>scheduler.init()</code> 代码 : </summary>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, scheduler_config: SchedulerConfig, cache_config: CacheConfig, lora_config: <span class="type">Optional</span>[LoRAConfig],</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.scheduler_config = scheduler_config</span><br><span class="line">        self.cache_config = cache_config</span><br><span class="line">        self.prompt_limit = <span class="built_in">min</span>(</span><br><span class="line">                self.scheduler_config.max_model_len,</span><br><span class="line">                self.scheduler_config.max_num_batched_tokens)</span><br><span class="line">        <span class="comment"># Create the block space manager.</span></span><br><span class="line">        self.block_manager = BlockSpaceManagerImpl(</span><br><span class="line">            block_size=self.cache_config.block_size,</span><br><span class="line">            num_gpu_blocks=self.cache_config.num_gpu_blocks,</span><br><span class="line">            num_cpu_blocks=self.cache_config.num_cpu_blocks,</span><br><span class="line">            sliding_window=self.cache_config.sliding_window,</span><br><span class="line">            enable_caching=self.cache_config.enable_prefix_caching)</span><br><span class="line">        self.waiting: Deque[SequenceGroup] = deque()</span><br><span class="line">        self.running: Deque[SequenceGroup] = deque()</span><br><span class="line">        self.swapped: Deque[SequenceGroup] = deque()</span><br></pre></td></tr></table></figure>
</details> -->
<p>内部主要组件为：</p>
<ol>
<li>
<p><code>self.waiting, self.running, self.swapped: Deque[SequenceGroup]</code> ：三个队列</p>
</li>
<li>
<p><code>self.block_manager</code> ：逻辑块到内存块映射，管理 logical token block 和 physical token block 之间的映射关系。</p>
<p><code>block_manager</code> 初始化方法</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BlockSpaceManager</span>: <span class="string">&quot;&quot;&quot;Manages the mapping between logical and physical token blocks.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block_size: <span class="built_in">int</span>, num_gpu_blocks: <span class="built_in">int</span>, num_cpu_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                    watermark: <span class="built_in">float</span> = <span class="number">0.01</span>, sliding_window: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line">        self.watermark_blocks = <span class="built_in">int</span>(watermark * num_gpu_blocks)</span><br><span class="line">        self.gpu_allocator = BlockAllocator(Device.GPU, block_size, num_gpu_blocks)</span><br><span class="line">        self.cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)</span><br><span class="line">        <span class="comment"># Mapping: seq_id -&gt; BlockTable.</span></span><br><span class="line">        self.block_tables: <span class="type">Dict</span>[<span class="built_in">int</span>, BlockTable] = &#123;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>内部还有一个 <code>policy</code> ，一般为 <code>fcfs</code> 方法： <code>PolicyFactory.get_policy(policy_name=&quot;fcfs&quot;)</code></p>
</li>
</ol>
<h2 id="worker"><a class="markdownIt-Anchor" href="#worker"></a> Worker</h2>
<p>model_executor.Worker ：推理组件 - 负责「KV cache的管理」和「模型推理」，负责使用模型对被调度的请求进行推理<br />
Worker 内部组件为：</p>
<ol>
<li>
<p><code>Worker.model_runner : ModelRunner</code> ：对 Model 的封装，主要负责模型的推理。</p>
</li>
<li>
<p><code>Worker.cache_engine : CacheEngine</code> ：用于 KV cache 的管理，它的职责是初始化和管理 GPU 和 CPU KV cache。</p>
<ul>
<li>对 KV cache 的管理是通过提前将一大块空间（显存或内存）划分成多个块（block），每块有 <code>block_size</code> 个 slot，每个 slot 存放一个 token 对应的 K V 值。</li>
<li>在初始化 Worker 时，不会初始化 <code>Worker.cache_engine</code> ，<br />
真正的初始化是在 <code>LLMEngine.__init__()</code> 部分中的 <code>LLMEngine._initialize_kv_caches()</code> 函数:
<ol>
<li>首先使用 <code>LLMEngine.model_executor.determine_num_available_blocks()</code> 方法计算 <code>num_gpu_blocks, num_cpu_blocks</code></li>
<li>根据计算后的 block size 进行初始化分配  <code>self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)</code> 。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><code>Worker.gpu_cache</code></p>
</li>
</ol>
<p>初始化位于 <code>LLMEngine.from_engine_args()</code> &gt;&gt; <code>engine = cls(..., executor_class=executor_class, ...)</code><br />
&gt;&gt; 参数 <code>executor_class</code> &gt;&gt; 最终 Worker 为 <code>LLMEngine.model_executor.driver_worker : Worker</code></p>
<h2 id="核心方法"><a class="markdownIt-Anchor" href="#核心方法"></a> 核心方法</h2>
<ol>
<li>
<p><code>add_request()</code></p>
<ol>
<li>把每个 prompt 包装成一个 <code>SequenceGroup</code> 对象<br />
为什么需要 <code>SequenceGroup</code> -&gt; 为了 beam search 或者 parallel sampling ，一个 <code>seq_group</code>中的所有 sequence 共享1个 prompt 。</li>
<li>把包装成 <code>SequenceGroup</code> 对象的数据加入调度器（Scheduler）的 「waiting队列」，等待处理。</li>
</ol>
</li>
<li>
<p><code>_run_engine()</code> ：执行推理，只要调度器的 「waiting/running/swapped队列」 非空，我们就认为此时这批batch还没有做完推理，这时我们就会调用 <code>LLMEngine.step()</code> 函数，来完成1次调度以决定要送哪些数据去做推理。</p>
</li>
<li>
<p><code>step()</code> ： 负责执行1次推理过程，prefill 或者 generate 。</p>
</li>
<li>
<p><code>abort_request()</code> ： 在推理过程中，并不是所有的请求都能有返回结果。比如客户端断开连接时，这个请求的推理就可以终止了。</p>
</li>
</ol>
<h2 id="加载模型与预分配显存"><a class="markdownIt-Anchor" href="#加载模型与预分配显存"></a> 加载模型与预分配显存</h2>
<p>初始化 <code>LLMEngine</code> 时：需要首先进行模型加载，和预分配显存。</p>
<ol>
<li>
<p>模型加载：<br />
<img src="https://pic3.zhimg.com/80/v2-83b8cc298c0f2369e75ba3f2f7ef06a2_1440w.webp" alt="" /></p>
</li>
<li>
<p>预分配显存实验：</p>
<p><img src="https://pic3.zhimg.com/80/v2-9df49b61fa249c0a6ff05a8d64858366_1440w.webp" alt="" /></p>
<p>通过模拟实验 <code>profile_num_available_blocks</code> 的方式，来决定gpu/cpu上到底有多少个KV cache物理块可以分配给后续的请求们做推理。</p>
<p>过程：根据超参数<br />
<code>max_num_seqs</code>（1个推理阶段中，最多能处理的seq数量）<br />
<code>max_num_batched_tokens</code>（1个推理阶段中，LLMEngine最多能处理的token数量）<br />
计算得到 平均一个seq要处理token 数量：<code>max_num_batched_tokens // max_num_seqs</code> ，余数部分我们默认放在第一个seq中。</p>
<p>计算1次推理过程中，可以分配多少的显存给 KV cache：<br />
gpu总显存 - <code>KV cache=0</code> 时做1次推理时的显存占用（包含了模型大小和激活峰值）</p>
<p>于是就可以得到 KV cache 的最大占用显存数量</p>
<p>于是得到：</p>
<p><strong>总物理块数量</strong> = 分配给KV Cache的显存大小 / 物理块尺寸大小 ，其中“大小”的单位是bytes。<br />
物理块尺寸大小（block_size），也即一个物理块有多少个槽位，是可以由用户自定义的，vLLM推荐的默认值是 <code>block_size=16</code> 。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache_block_size = V_cache_block_size = K_cache_block_size</span><br><span class="line">V_cache_block_size = K_cache_block_size = block_size * num_heads * head_size * num_layers * dtype_size</span><br></pre></td></tr></table></figure>
<p>CPU上物理块总数也是同理，但与GPU不同的是，它不需要做模拟实验。CPU上可用的内存总数是用户通过参数传进来的（默认是4G）。</p>
</li>
<li>
<p>预分配显存 <code>_allocate_kv_cache</code> ：将预分配的 KV Cache 加载到 gpu 上</p>
<p><img src="https://pic4.zhimg.com/80/v2-e8b544bc41596a0de9f814d47ca347cf_1440w.webp" alt="" /></p>
<p>当确定好KV Cache block的大小后，我们就可以创建 empty tensor，将其先放置到gpu上 ，实现显存的预分配。以后这块显存就是专门用来做KV Cache的了。</p>
</li>
</ol>
<h3 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h3>
<p>在初始化 <code>LLM.LLMEngine</code> 时，<code>LLMEngine</code> 会调用 <code>self._initialize_kv_caches()</code> 方法进行预分配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_kv_caches</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize the KV cache in the worker(s).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The workers will determine the number of blocks in both the GPU cache</span></span><br><span class="line"><span class="string">    and the swap CPU cache.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_gpu_blocks, num_cpu_blocks = (</span><br><span class="line">        self.model_executor.determine_num_available_blocks())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 支持自定义 num_gpu_blocks</span></span><br><span class="line">    <span class="keyword">if</span> self.cache_config.num_gpu_blocks_override <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override</span><br><span class="line">        logger.info(<span class="string">f&quot;Overriding <span class="subst">&#123;num_gpu_blocks=&#125;</span> with &quot;</span></span><br><span class="line">                    <span class="string">f&quot;<span class="subst">&#123;num_gpu_blocks_override=&#125;</span>&quot;</span>)</span><br><span class="line">        num_gpu_blocks = num_gpu_blocks_override</span><br><span class="line"></span><br><span class="line">    self.cache_config.num_gpu_blocks = num_gpu_blocks</span><br><span class="line">    self.cache_config.num_cpu_blocks = num_cpu_blocks</span><br><span class="line"></span><br><span class="line">    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)</span><br></pre></td></tr></table></figure>
<p>最重要的方法为： <code>vllm / worker / worker / determine_num_available_blocks()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.inference_mode()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">determine_num_available_blocks</span>(<span class="params">self</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Profiles the peak memory usage of the model to determine how many KV blocks may be allocated without OOMs.</span></span><br><span class="line"><span class="string">    The engine will first conduct a profiling of the existing memory usage.</span></span><br><span class="line"><span class="string">    Then, it calculate the maximum possible number of GPU and CPU blocks that can be allocated with the remaining free memory.</span></span><br><span class="line"><span class="string">    You may limit the usage of GPU memory by adjusting the `gpu_memory_utilization` parameter.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Execute a forward pass with dummy inputs to profile the memory usage of the model.</span></span><br><span class="line">    <span class="comment"># 根据预先设置的 max_num_batched_tokens 和 max_num_seqs 模拟一次 forward</span></span><br><span class="line">    self.model_runner.profile_run()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算剩余的最大显存 peak_memory</span></span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()</span><br><span class="line">    peak_memory = self.init_gpu_memory - free_gpu_memory</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据每个 block 的大小得到 block size</span></span><br><span class="line">    cache_block_size = self.get_cache_block_size_bytes()</span><br><span class="line">    num_gpu_blocks = <span class="built_in">int</span>( <span class="comment"># 得到剩余 block 数量</span></span><br><span class="line">        <span class="comment"># self.cache_config.gpu_memory_utilization 为我们设定 GPU 最大占用显存的量</span></span><br><span class="line">        <span class="comment"># total_gpu_memory * self.cache_config.gpu_memory_utilization - peak_memory 为目标剩余显存数量</span></span><br><span class="line">        (total_gpu_memory * self.cache_config.gpu_memory_utilization - peak_memory)</span><br><span class="line">        // cache_block_size)</span><br><span class="line">    <span class="comment"># 计算 CPU 可以容纳的最大 block size -&gt; for swap</span></span><br><span class="line">    num_cpu_blocks = <span class="built_in">int</span>(self.cache_config.swap_space_bytes // cache_block_size)</span><br><span class="line"></span><br><span class="line">    num_gpu_blocks = <span class="built_in">max</span>(num_gpu_blocks, <span class="number">0</span>)</span><br><span class="line">    num_cpu_blocks = <span class="built_in">max</span>(num_cpu_blocks, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    gc.collect()</span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line">    <span class="keyword">return</span> num_gpu_blocks, num_cpu_blocks</span><br></pre></td></tr></table></figure>
<p>这里需要注意下<code> worker/cache_engine.py</code> 中的 <code>get_cache_block_size</code> 方法，也就是如何获得 block size</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cache_block_size</span>(<span class="params"></span></span><br><span class="line"><span class="params">    cache_config: CacheConfig, model_config: ModelConfig, parallel_config: ParallelConfig,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    head_size = model_config.get_head_size()</span><br><span class="line">    num_heads = model_config.get_num_kv_heads(parallel_config)</span><br><span class="line">    num_layers = model_config.get_num_layers(parallel_config)</span><br><span class="line"></span><br><span class="line">    key_cache_block = cache_config.block_size * num_heads * head_size</span><br><span class="line">    value_cache_block = key_cache_block</span><br><span class="line">    total = num_layers * (key_cache_block + value_cache_block)</span><br><span class="line">    <span class="keyword">if</span> cache_config.cache_dtype == <span class="string">&quot;auto&quot;</span>:</span><br><span class="line">        dtype = model_config.dtype</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]</span><br><span class="line">    dtype_size = _get_dtype_size(dtype)</span><br><span class="line">    <span class="keyword">return</span> dtype_size * total</span><br></pre></td></tr></table></figure>
<p>每一个 token 对应的显存大小为 <code>num_layers * (key_cache_block + value_cache_block)</code> 。<br />
k，v 的显存单位计算是不一样的， <code>key_cache_block</code> &lt;-&gt; <code>value_cache_block</code> 。</p>
<h1 id="scheduler-调度"><a class="markdownIt-Anchor" href="#scheduler-调度"></a> Scheduler 调度</h1>
<p><img src="https://pic2.zhimg.com/80/v2-ab509dc88461321fe2b323fe04b43285_1440w.webp" alt="" /></p>
<p><img src="image.png" alt="alt text" /></p>
<p>在每 1 个推理阶段中，调度器（Scheduler）决定哪些请求可以参与推理，并为这些请求做好 <strong>逻辑块-&gt;物理块</strong> 的映射。</p>
<p>每 1 个推理阶段的定义是：prefill算1个推理阶段，decode算1个推理阶段，无法在同一个 step 中执行。</p>
<h1 id="related-blog-and-source"><a class="markdownIt-Anchor" href="#related-blog-and-source"></a> Related Blog and Source</h1>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692540949">图解大模型计算加速系列：vLLM源码解析2，调度器策略(Scheduler)</a></p>
<p><a target="_blank" rel="noopener" href="https://ploomber.io/blog/vllm-deploy/">Deploying vLLM: a Step-by-Step Guide</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/26/NLP/LLM/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8E%8B%E7%BC%A92LLMLingua/" rel="prev" title="上下文压缩2：LLMLingua">
      <i class="fa fa-chevron-left"></i> 上下文压缩2：LLMLingua
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/04/12/NLP/LLM/MiniCPM%EF%BC%9A%E6%8F%AD%E7%A4%BA%E7%AB%AF%E4%BE%A7%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%97%A0%E9%99%90%E6%BD%9C%E5%8A%9B/" rel="next" title="MiniCPM：揭示端侧大语言模型的无限潜力">
      MiniCPM：揭示端侧大语言模型的无限潜力 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91"><span class="nav-number">1.</span> <span class="nav-text"> 代码逻辑</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#vllm%E4%BB%A3%E7%A0%81%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text"> vLLM代码整体架构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#llm-%E7%A6%BB%E7%BA%BF"><span class="nav-number">2.</span> <span class="nav-text"> LLM 离线</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#llmengine"><span class="nav-number">3.</span> <span class="nav-text"> LLMEngine</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#scheduler"><span class="nav-number">3.1.</span> <span class="nav-text"> Scheduler</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#worker"><span class="nav-number">3.2.</span> <span class="nav-text"> Worker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text"> 核心方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%A2%84%E5%88%86%E9%85%8D%E6%98%BE%E5%AD%98"><span class="nav-number">3.4.</span> <span class="nav-text"> 加载模型与预分配显存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">3.4.1.</span> <span class="nav-text"> 代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scheduler-%E8%B0%83%E5%BA%A6"><span class="nav-number">4.</span> <span class="nav-text"> Scheduler 调度</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#related-blog-and-source"><span class="nav-number">5.</span> <span class="nav-text"> Related Blog and Source</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">A foolish, slow learner.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">349</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YiandLi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YiandLi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yiliiiii" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yiliiiii" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">599k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">49:56</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
